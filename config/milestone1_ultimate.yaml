project:
  name: "milestone1_ultimate_m2"
  target_f1: 0.94

model:
  name: "roberta-base"
  num_labels: 2
  max_length: 256
  dropout: 0.15

training:
  device: "mps"
  batch_size: 16
  learning_rate: 1e-5            # lower for stability on MPS
  epochs_per_chunk: 6
  gradient_accumulation_steps: 4
  warmup_steps: 1000
  weight_decay: 0.01
  mixed_precision: false         # AMP off avoids NaNs on MPS

hardware:
  dataloader_num_workers: 0      # avoid fork + tokenizers issues on macOS
  pin_memory: false              # not useful/supported on MPS
  gradient_checkpointing: true

paths:
  model_dir: "models/milestone_01/final"
  checkpoint_dir: "models/milestone_01/checkpoints"
  ensemble_dir: "models/milestone_01/ensemble"
  output_dir: "outputs/milestone1"

data:
  chunks_dir: "data/processed/milestone1"
  train_pattern: "enhanced_ultimate_train_chunk_*.csv"
  test_pattern: "enhanced_ultimate_test_chunk_*.csv"
