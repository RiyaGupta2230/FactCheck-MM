{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a00d6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# FactCheck-MM Error Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Overview\\n\",\n",
    "    \"This notebook provides comprehensive error analysis for FactCheck-MM models including:\\n\",\n",
    "    \"- Misclassification pattern identification\\n\",\n",
    "    \"- Multimodal vs text-only error comparison\\n\",\n",
    "    \"- Task-specific failure case analysis\\n\",\n",
    "    \"- Error distribution across datasets\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Method\\n\",\n",
    "    \"We analyze model errors across three tasks:\\n\",\n",
    "    \"- **Sarcasm Detection**: False positives/negatives, multimodal benefits analysis\\n\",\n",
    "    \"- **Paraphrasing**: Low-quality generations, semantic preservation failures\\n\",\n",
    "    \"- **Fact Verification**: Evidence retrieval vs classification errors\\n\",\n",
    "    \"\\n\",\n",
    "    \"Each error type is systematically categorized and analyzed for patterns.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Setup and imports\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from sklearn.metrics import confusion_matrix, classification_report\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add project root to path\\n\",\n",
    "    \"project_root = Path().cwd().parent if Path().cwd().name == 'notebooks' else Path().cwd()\\n\",\n",
    "    \"sys.path.insert(0, str(project_root))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project utilities\\n\",\n",
    "    \"from shared.utils.metrics import MetricsComputer, calculate_error_metrics\\n\",\n",
    "    \"from shared.utils.visualization import plot_error_distribution, create_error_heatmap\\n\",\n",
    "    \"from shared.datasets.unified_loader import UnifiedDatasetLoader\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set style\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create output directory\\n\",\n",
    "    \"output_dir = project_root / 'outputs' / 'notebooks'\\n\",\n",
    "    \"output_dir.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Project root: {project_root}\\\")\\n\",\n",
    "    \"print(f\\\"Output directory: {output_dir}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Generate Mock Prediction Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate mock prediction data for error analysis\\n\",\n",
    "    \"def generate_mock_predictions():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Generate mock prediction data with realistic error patterns.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Mock sarcasm detection predictions\\n\",\n",
    "    \"    n_sarcasm_samples = 1000\\n\",\n",
    "    \"    sarcasm_data = {\\n\",\n",
    "    \"        'text': [],\\n\",\n",
    "    \"        'true_label': np.random.choice([0, 1], n_sarcasm_samples, p=[0.6, 0.4]),\\n\",\n",
    "    \"        'text_model_pred': [],\\n\",\n",
    "    \"        'multimodal_pred': [],\\n\",\n",
    "    \"        'confidence_text': [],\\n\",\n",
    "    \"        'confidence_multimodal': [],\\n\",\n",
    "    \"        'dataset': np.random.choice(['SARC', 'MMSD2', 'MUStARD', 'UR_FUNNY'], n_sarcasm_samples),\\n\",\n",
    "    \"        'has_audio': np.random.choice([True, False], n_sarcasm_samples, p=[0.3, 0.7]),\\n\",\n",
    "    \"        'has_image': np.random.choice([True, False], n_sarcasm_samples, p=[0.4, 0.6]),\\n\",\n",
    "    \"        'text_length': np.random.normal(25, 10, n_sarcasm_samples)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate realistic text samples\\n\",\n",
    "    \"    sarcastic_texts = [\\n\",\n",
    "    \"        \\\"Oh great, another Monday morning meeting\\\",\\n\",\n",
    "    \"        \\\"Yeah, because that's exactly what I needed today\\\",\\n\",\n",
    "    \"        \\\"Wow, such amazing weather for a picnic\\\",\\n\",\n",
    "    \"        \\\"Perfect timing as always\\\",\\n\",\n",
    "    \"        \\\"Just what the doctor ordered\\\",\\n\",\n",
    "    \"        \\\"Fantastic, my favorite kind of surprise\\\",\\n\",\n",
    "    \"        \\\"Oh wonderful, more homework\\\",\\n\",\n",
    "    \"        \\\"Brilliant idea, absolutely brilliant\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    non_sarcastic_texts = [\\n\",\n",
    "    \"        \\\"I really enjoyed the movie last night\\\",\\n\",\n",
    "    \"        \\\"Thank you for your help today\\\",\\n\",\n",
    "    \"        \\\"The weather is beautiful for a walk\\\",\\n\",\n",
    "    \"        \\\"I'm looking forward to the weekend\\\",\\n\",\n",
    "    \"        \\\"This restaurant has great food\\\",\\n\",\n",
    "    \"        \\\"I appreciate your feedback\\\",\\n\",\n",
    "    \"        \\\"The meeting was very productive\\\",\\n\",\n",
    "    \"        \\\"I love spending time with family\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create text samples\\n\",\n",
    "    \"    for i in range(n_sarcasm_samples):\\n\",\n",
    "    \"        if sarcasm_data['true_label'][i] == 1:\\n\",\n",
    "    \"            base_text = np.random.choice(sarcastic_texts)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            base_text = np.random.choice(non_sarcastic_texts)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add some variation\\n\",\n",
    "    \"        variations = [\\\" today\\\", \\\" really\\\", \\\" again\\\", \\\"\\\"]\\n\",\n",
    "    \"        sarcasm_data['text'].append(base_text + np.random.choice(variations))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate predictions with realistic error patterns\\n\",\n",
    "    \"    for i in range(n_sarcasm_samples):\\n\",\n",
    "    \"        true_label = sarcasm_data['true_label'][i]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Text-only model (less accurate, especially for subtle sarcasm)\\n\",\n",
    "    \"        text_accuracy = 0.75\\n\",\n",
    "    \"        if np.random.random() < text_accuracy:\\n\",\n",
    "    \"            text_pred = true_label\\n\",\n",
    "    \"            confidence_text = np.random.uniform(0.7, 0.95)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            text_pred = 1 - true_label\\n\",\n",
    "    \"            confidence_text = np.random.uniform(0.5, 0.8)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Multimodal model (more accurate, especially with audio/video cues)\\n\",\n",
    "    \"        multimodal_accuracy = 0.85\\n\",\n",
    "    \"        # Bonus accuracy if multimodal cues available\\n\",\n",
    "    \"        if sarcasm_data['has_audio'][i] or sarcasm_data['has_image'][i]:\\n\",\n",
    "    \"            multimodal_accuracy += 0.05\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if np.random.random() < multimodal_accuracy:\\n\",\n",
    "    \"            multimodal_pred = true_label\\n\",\n",
    "    \"            confidence_multimodal = np.random.uniform(0.8, 0.98)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            multimodal_pred = 1 - true_label\\n\",\n",
    "    \"            confidence_multimodal = np.random.uniform(0.5, 0.85)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        sarcasm_data['text_model_pred'].append(text_pred)\\n\",\n",
    "    \"        sarcasm_data['multimodal_pred'].append(multimodal_pred)\\n\",\n",
    "    \"        sarcasm_data['confidence_text'].append(confidence_text)\\n\",\n",
    "    \"        sarcasm_data['confidence_multimodal'].append(confidence_multimodal)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Mock fact verification predictions\\n\",\n",
    "    \"    n_fact_samples = 800\\n\",\n",
    "    \"    fact_data = {\\n\",\n",
    "    \"        'claim': [],\\n\",\n",
    "    \"        'evidence': [],\\n\",\n",
    "    \"        'true_label': np.random.choice([0, 1, 2], n_fact_samples, p=[0.4, 0.3, 0.3]),  # SUPPORTS, REFUTES, NEI\\n\",\n",
    "    \"        'predicted_label': [],\\n\",\n",
    "    \"        'confidence': [],\\n\",\n",
    "    \"        'evidence_quality': np.random.uniform(0.3, 1.0, n_fact_samples),\\n\",\n",
    "    \"        'claim_complexity': np.random.uniform(0.2, 0.9, n_fact_samples),\\n\",\n",
    "    \"        'dataset': np.random.choice(['FEVER', 'LIAR'], n_fact_samples)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate fact verification samples\\n\",\n",
    "    \"    claims = [\\n\",\n",
    "    \"        \\\"The Earth is the third planet from the Sun\\\",\\n\",\n",
    "    \"        \\\"Water boils at 100 degrees Celsius\\\",\\n\",\n",
    "    \"        \\\"Shakespeare wrote Romeo and Juliet\\\",\\n\",\n",
    "    \"        \\\"The Great Wall of China is visible from space\\\",\\n\",\n",
    "    \"        \\\"Humans use only 10% of their brain\\\",\\n\",\n",
    "    \"        \\\"Lightning never strikes the same place twice\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    evidences = [\\n\",\n",
    "    \"        \\\"Scientific sources confirm this astronomical fact\\\",\\n\",\n",
    "    \"        \\\"Physics textbooks state this temperature\\\",\\n\",\n",
    "    \"        \\\"Literary records show this authorship\\\",\\n\",\n",
    "    \"        \\\"NASA studies contradict this popular belief\\\",\\n\",\n",
    "    \"        \\\"Neuroscience research disproves this myth\\\",\\n\",\n",
    "    \"        \\\"Meteorological data shows multiple strikes\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i in range(n_fact_samples):\\n\",\n",
    "    \"        fact_data['claim'].append(np.random.choice(claims) + f\\\" (variant {i%10})\\\")\\n\",\n",
    "    \"        fact_data['evidence'].append(np.random.choice(evidences))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Prediction accuracy depends on evidence quality and claim complexity\\n\",\n",
    "    \"        accuracy = 0.6 + 0.3 * fact_data['evidence_quality'][i] - 0.2 * fact_data['claim_complexity'][i]\\n\",\n",
    "    \"        accuracy = np.clip(accuracy, 0.4, 0.9)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        true_label = fact_data['true_label'][i]\\n\",\n",
    "    \"        if np.random.random() < accuracy:\\n\",\n",
    "    \"            pred_label = true_label\\n\",\n",
    "    \"            confidence = np.random.uniform(0.7, 0.95)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            pred_label = np.random.choice([l for l in [0, 1, 2] if l != true_label])\\n\",\n",
    "    \"            confidence = np.random.uniform(0.4, 0.8)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fact_data['predicted_label'].append(pred_label)\\n\",\n",
    "    \"        fact_data['confidence'].append(confidence)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Mock paraphrasing evaluation\\n\",\n",
    "    \"    n_para_samples = 500\\n\",\n",
    "    \"    para_data = {\\n\",\n",
    "    \"        'source_text': [],\\n\",\n",
    "    \"        'reference_paraphrase': [],\\n\",\n",
    "    \"        'generated_paraphrase': [],\\n\",\n",
    "    \"        'bleu_score': np.random.uniform(0.2, 0.8, n_para_samples),\\n\",\n",
    "    \"        'rouge_score': np.random.uniform(0.3, 0.9, n_para_samples),\\n\",\n",
    "    \"        'semantic_similarity': np.random.uniform(0.4, 0.95, n_para_samples),\\n\",\n",
    "    \"        'fluency_score': np.random.uniform(0.5, 1.0, n_para_samples),\\n\",\n",
    "    \"        'source_length': np.random.normal(20, 8, n_para_samples),\\n\",\n",
    "    \"        'dataset': np.random.choice(['ParaNMT', 'MRPC', 'Quora'], n_para_samples)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    source_texts = [\\n\",\n",
    "    \"        \\\"The weather today is very nice and sunny\\\",\\n\",\n",
    "    \"        \\\"I need to go to the store to buy groceries\\\",\\n\",\n",
    "    \"        \\\"The movie we watched last night was entertaining\\\",\\n\",\n",
    "    \"        \\\"She completed her homework before dinner\\\",\\n\",\n",
    "    \"        \\\"The cat is sleeping on the comfortable couch\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i in range(n_para_samples):\\n\",\n",
    "    \"        source = np.random.choice(source_texts) + f\\\" (example {i%20})\\\"\\n\",\n",
    "    \"        para_data['source_text'].append(source)\\n\",\n",
    "    \"        para_data['reference_paraphrase'].append(f\\\"Reference paraphrase of: {source}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Generate paraphrase quality based on scores\\n\",\n",
    "    \"        quality = para_data['semantic_similarity'][i] * para_data['fluency_score'][i]\\n\",\n",
    "    \"        if quality > 0.7:\\n\",\n",
    "    \"            para_data['generated_paraphrase'].append(f\\\"High-quality paraphrase: {source}\\\")\\n\",\n",
    "    \"        elif quality > 0.5:\\n\",\n",
    "    \"            para_data['generated_paraphrase'].append(f\\\"Medium paraphrase: {source}\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            para_data['generated_paraphrase'].append(f\\\"Low-quality generation: {source[:20]}...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return pd.DataFrame(sarcasm_data), pd.DataFrame(fact_data), pd.DataFrame(para_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate mock data\\n\",\n",
    "    \"df_sarcasm, df_fact, df_para = generate_mock_predictions()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Generated prediction data:\\\")\\n\",\n",
    "    \"print(f\\\"  Sarcasm detection: {len(df_sarcasm)} samples\\\")\\n\",\n",
    "    \"print(f\\\"  Fact verification: {len(df_fact)} samples\\\")\\n\",\n",
    "    \"print(f\\\"  Paraphrasing: {len(df_para)} samples\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Sarcasm Detection Error Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze sarcasm detection errors\\n\",\n",
    "    \"def analyze_sarcasm_errors(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Comprehensive analysis of sarcasm detection errors.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Calculate error types\\n\",\n",
    "    \"    df['text_correct'] = (df['true_label'] == df['text_model_pred'])\\n\",\n",
    "    \"    df['multimodal_correct'] = (df['true_label'] == df['multimodal_pred'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Error categorization\\n\",\n",
    "    \"    df['error_type'] = 'Correct (Both)'\\n\",\n",
    "    \"    df.loc[(~df['text_correct']) & (~df['multimodal_correct']), 'error_type'] = 'Error (Both)'\\n\",\n",
    "    \"    df.loc[(~df['text_correct']) & (df['multimodal_correct']), 'error_type'] = 'Text Error Only'\\n\",\n",
    "    \"    df.loc[(df['text_correct']) & (~df['multimodal_correct']), 'error_type'] = 'Multimodal Error Only'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # False positive/negative analysis\\n\",\n",
    "    \"    df['text_fp'] = (df['true_label'] == 0) & (df['text_model_pred'] == 1)\\n\",\n",
    "    \"    df['text_fn'] = (df['true_label'] == 1) & (df['text_model_pred'] == 0)\\n\",\n",
    "    \"    df['multimodal_fp'] = (df['true_label'] == 0) & (df['multimodal_pred'] == 1)\\n\",\n",
    "    \"    df['multimodal_fn'] = (df['true_label'] == 1) & (df['multimodal_pred'] == 0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_sarcasm = analyze_sarcasm_errors(df_sarcasm)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create comprehensive error analysis visualization\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 1: Error Type Distribution\\n\",\n",
    "    \"ax1 = axes[0, 0]\\n\",\n",
    "    \"error_counts = df_sarcasm['error_type'].value_counts()\\n\",\n",
    "    \"colors = ['#2E8B57', '#FF6B6B', '#FFD93D', '#4ECDC4']\\n\",\n",
    "    \"wedges, texts, autotexts = ax1.pie(error_counts.values, labels=error_counts.index, \\n\",\n",
    "    \"                                   autopct='%1.1f%%', colors=colors, startangle=90)\\n\",\n",
    "    \"ax1.set_title('Sarcasm Detection Error Distribution', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 2: Text vs Multimodal Accuracy by Dataset\\n\",\n",
    "    \"ax2 = axes[0, 1]\\n\",\n",
    "    \"dataset_accuracy = df_sarcasm.groupby('dataset').agg({\\n\",\n",
    "    \"    'text_correct': 'mean',\\n\",\n",
    "    \"    'multimodal_correct': 'mean'\\n\",\n",
    "    \"}).reset_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"x_pos = np.arange(len(dataset_accuracy))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars1 = ax2.bar(x_pos - width/2, dataset_accuracy['text_correct'], width,\\n\",\n",
    "    \"                label='Text-only', color='lightcoral', alpha=0.8)\\n\",\n",
    "    \"bars2 = ax2.bar(x_pos + width/2, dataset_accuracy['multimodal_correct'], width,\\n\",\n",
    "    \"                label='Multimodal', color='skyblue', alpha=0.8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax2.set_title('Model Accuracy by Dataset', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax2.set_xlabel('Dataset')\\n\",\n",
    "    \"ax2.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax2.set_xticks(x_pos)\\n\",\n",
    "    \"ax2.set_xticklabels(dataset_accuracy['dataset'])\\n\",\n",
    "    \"ax2.legend()\\n\",\n",
    "    \"ax2.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bars in [bars1, bars2]:\\n\",\n",
    "    \"    for bar in bars:\\n\",\n",
    "    \"        height = bar.get_height()\\n\",\n",
    "    \"        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\\n\",\n",
    "    \"                f'{height:.3f}', ha='center', va='bottom', fontsize=10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 3: Error Analysis by Multimodal Availability\\n\",\n",
    "    \"ax3 = axes[1, 0]\\n\",\n",
    "    \"multimodal_benefit = df_sarcasm.copy()\\n\",\n",
    "    \"multimodal_benefit['has_multimodal'] = multimodal_benefit['has_audio'] | multimodal_benefit['has_image']\\n\",\n",
    "    \"\\n\",\n",
    "    \"benefit_analysis = multimodal_benefit.groupby('has_multimodal').agg({\\n\",\n",
    "    \"    'text_correct': 'mean',\\n\",\n",
    "    \"    'multimodal_correct': 'mean'\\n\",\n",
    "    \"}).reset_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"benefit_analysis['improvement'] = benefit_analysis['multimodal_correct'] - benefit_analysis['text_correct']\\n\",\n",
    "    \"\\n\",\n",
    "    \"categories = ['Text-only Available', 'Multimodal Available']\\n\",\n",
    "    \"improvements = benefit_analysis['improvement'].values\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars = ax3.bar(categories, improvements, color=['#FF9999', '#66B2FF'], alpha=0.7)\\n\",\n",
    "    \"ax3.set_title('Multimodal Improvement by Data Availability', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax3.set_ylabel('Accuracy Improvement')\\n\",\n",
    "    \"ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bar, improvement in zip(bars, improvements):\\n\",\n",
    "    \"    height = bar.get_height()\\n\",\n",
    "    \"    ax3.text(bar.get_x() + bar.get_width()/2., height + (0.005 if height >= 0 else -0.015),\\n\",\n",
    "    \"            f'{improvement:.3f}', ha='center', va='bottom' if height >= 0 else 'top',\\n\",\n",
    "    \"            fontsize=12, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 4: Confidence vs Correctness\\n\",\n",
    "    \"ax4 = axes[1, 1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create confidence bins\\n\",\n",
    "    \"df_sarcasm['text_conf_bin'] = pd.cut(df_sarcasm['confidence_text'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\\n\",\n",
    "    \"df_sarcasm['mm_conf_bin'] = pd.cut(df_sarcasm['confidence_multimodal'], bins=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"text_conf_acc = df_sarcasm.groupby('text_conf_bin')['text_correct'].mean()\\n\",\n",
    "    \"mm_conf_acc = df_sarcasm.groupby('mm_conf_bin')['multimodal_correct'].mean()\\n\",\n",
    "    \"\\n\",\n",
    "    \"x_pos = np.arange(len(text_conf_acc))\\n\",\n",
    "    \"ax4.plot(x_pos, text_conf_acc.values, marker='o', label='Text-only', linewidth=2, markersize=6)\\n\",\n",
    "    \"ax4.plot(x_pos, mm_conf_acc.values, marker='s', label='Multimodal', linewidth=2, markersize=6)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax4.set_title('Accuracy vs Confidence Level', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax4.set_xlabel('Confidence Level')\\n\",\n",
    "    \"ax4.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax4.set_xticks(x_pos)\\n\",\n",
    "    \"ax4.set_xticklabels(text_conf_acc.index)\\n\",\n",
    "    \"ax4.legend()\\n\",\n",
    "    \"ax4.grid(True, alpha=0.3)\\n\",\n",
    "    \"ax4.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig(output_dir / 'sarcasm_error_analysis.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Sarcasm Detection Error Analysis:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"Text-only accuracy: {df_sarcasm['text_correct'].mean():.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Multimodal accuracy: {df_sarcasm['multimodal_correct'].mean():.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Multimodal improvement: {df_sarcasm['multimodal_correct'].mean() - df_sarcasm['text_correct'].mean():.3f}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nError type distribution:\\\")\\n\",\n",
    "    \"print(error_counts)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nSarcasm error analysis saved to: {output_dir / 'sarcasm_error_analysis.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Detailed false positive/negative analysis for sarcasm\\n\",\n",
    "    \"def analyze_fp_fn_patterns(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Analyze patterns in false positives and false negatives.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Extract error cases\\n\",\n",
    "    \"    text_fp_cases = df[df['text_fp']]\\n\",\n",
    "    \"    text_fn_cases = df[df['text_fn']]\\n\",\n",
    "    \"    mm_fp_cases = df[df['multimodal_fp']]\\n\",\n",
    "    \"    mm_fn_cases = df[df['multimodal_fn']]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Cases where multimodal helps\\n\",\n",
    "    \"    mm_helps_fp = df[df['text_fp'] & ~df['multimodal_fp']]  # Text FP, MM correct\\n\",\n",
    "    \"    mm_helps_fn = df[df['text_fn'] & ~df['multimodal_fn']]  # Text FN, MM correct\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nDetailed Error Pattern Analysis:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 40)\\n\",\n",
    "    \"    print(f\\\"Text-only False Positives: {len(text_fp_cases)} ({len(text_fp_cases)/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"    print(f\\\"Text-only False Negatives: {len(text_fn_cases)} ({len(text_fn_cases)/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"    print(f\\\"Multimodal False Positives: {len(mm_fp_cases)} ({len(mm_fp_cases)/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"    print(f\\\"Multimodal False Negatives: {len(mm_fn_cases)} ({len(mm_fn_cases)/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nMultimodal Benefits:\\\")\\n\",\n",
    "    \"    print(f\\\"MM fixes Text FP: {len(mm_helps_fp)} cases\\\")\\n\",\n",
    "    \"    print(f\\\"MM fixes Text FN: {len(mm_helps_fn)} cases\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show example cases\\n\",\n",
    "    \"    print(\\\"\\\\nExample Error Cases:\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 30)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(mm_helps_fp) > 0:\\n\",\n",
    "    \"        print(\\\"\\\\nCases where Multimodal fixed Text False Positives:\\\")\\n\",\n",
    "    \"        for i, (idx, row) in enumerate(mm_helps_fp.head(3).iterrows()):\\n\",\n",
    "    \"            print(f\\\"  {i+1}. Text: '{row['text'][:80]}...'\\\")\\n\",\n",
    "    \"            print(f\\\"     True: Non-sarcastic, Text pred: Sarcastic, MM pred: Non-sarcastic\\\")\\n\",\n",
    "    \"            print(f\\\"     Dataset: {row['dataset']}, Has audio: {row['has_audio']}, Has image: {row['has_image']}\\\")\\n\",\n",
    "    \"            print()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(mm_helps_fn) > 0:\\n\",\n",
    "    \"        print(\\\"\\\\nCases where Multimodal fixed Text False Negatives:\\\")\\n\",\n",
    "    \"        for i, (idx, row) in enumerate(mm_helps_fn.head(3).iterrows()):\\n\",\n",
    "    \"            print(f\\\"  {i+1}. Text: '{row['text'][:80]}...'\\\")\\n\",\n",
    "    \"            print(f\\\"     True: Sarcastic, Text pred: Non-sarcastic, MM pred: Sarcastic\\\")\\n\",\n",
    "    \"            print(f\\\"     Dataset: {row['dataset']}, Has audio: {row['has_audio']}, Has image: {row['has_image']}\\\")\\n\",\n",
    "    \"            print()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'text_fp': len(text_fp_cases),\\n\",\n",
    "    \"        'text_fn': len(text_fn_cases),\\n\",\n",
    "    \"        'mm_fp': len(mm_fp_cases),\\n\",\n",
    "    \"        'mm_fn': len(mm_fn_cases),\\n\",\n",
    "    \"        'mm_helps_fp': len(mm_helps_fp),\\n\",\n",
    "    \"        'mm_helps_fn': len(mm_helps_fn)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"error_stats = analyze_fp_fn_patterns(df_sarcasm)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Fact Verification Error Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze fact verification errors\\n\",\n",
    "    \"def analyze_fact_verification_errors(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Analyze fact verification errors by evidence quality and claim complexity.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    df['correct'] = (df['true_label'] == df['predicted_label'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Categorize by evidence quality and claim complexity\\n\",\n",
    "    \"    df['evidence_quality_cat'] = pd.cut(df['evidence_quality'], \\n\",\n",
    "    \"                                       bins=3, labels=['Low', 'Medium', 'High'])\\n\",\n",
    "    \"    df['claim_complexity_cat'] = pd.cut(df['claim_complexity'], \\n\",\n",
    "    \"                                       bins=3, labels=['Simple', 'Medium', 'Complex'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_fact = analyze_fact_verification_errors(df_fact)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create fact verification error analysis\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 1: Accuracy by Evidence Quality\\n\",\n",
    "    \"ax1 = axes[0, 0]\\n\",\n",
    "    \"evidence_accuracy = df_fact.groupby('evidence_quality_cat')['correct'].mean()\\n\",\n",
    "    \"bars = ax1.bar(evidence_accuracy.index, evidence_accuracy.values, \\n\",\n",
    "    \"               color=['#FF6B6B', '#FFD93D', '#2E8B57'], alpha=0.8)\\n\",\n",
    "    \"ax1.set_title('Accuracy by Evidence Quality', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax1.set_xlabel('Evidence Quality')\\n\",\n",
    "    \"ax1.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax1.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bar, acc in zip(bars, evidence_accuracy.values):\\n\",\n",
    "    \"    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\\n\",\n",
    "    \"            f'{acc:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 2: Accuracy by Claim Complexity\\n\",\n",
    "    \"ax2 = axes[0, 1]\\n\",\n",
    "    \"complexity_accuracy = df_fact.groupby('claim_complexity_cat')['correct'].mean()\\n\",\n",
    "    \"bars = ax2.bar(complexity_accuracy.index, complexity_accuracy.values,\\n\",\n",
    "    \"               color=['#2E8B57', '#FFD93D', '#FF6B6B'], alpha=0.8)\\n\",\n",
    "    \"ax2.set_title('Accuracy by Claim Complexity', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax2.set_xlabel('Claim Complexity')\\n\",\n",
    "    \"ax2.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax2.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bar, acc in zip(bars, complexity_accuracy.values):\\n\",\n",
    "    \"    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\\n\",\n",
    "    \"            f'{acc:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 3: Error Distribution by Label\\n\",\n",
    "    \"ax3 = axes[1, 0]\\n\",\n",
    "    \"label_names = ['SUPPORTS', 'REFUTES', 'NOT_ENOUGH_INFO']\\n\",\n",
    "    \"label_accuracy = df_fact.groupby('true_label')['correct'].mean()\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars = ax3.bar(range(len(label_names)), \\n\",\n",
    "    \"               [label_accuracy.get(i, 0) for i in range(len(label_names))],\\n\",\n",
    "    \"               color=['#4ECDC4', '#FF6B6B', '#FFD93D'], alpha=0.8)\\n\",\n",
    "    \"ax3.set_title('Accuracy by True Label', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax3.set_xlabel('True Label')\\n\",\n",
    "    \"ax3.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax3.set_xticks(range(len(label_names)))\\n\",\n",
    "    \"ax3.set_xticklabels(label_names, rotation=45, ha='right')\\n\",\n",
    "    \"ax3.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for i, bar in enumerate(bars):\\n\",\n",
    "    \"    height = bar.get_height()\\n\",\n",
    "    \"    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\\n\",\n",
    "    \"            f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 4: 2D Analysis - Evidence Quality vs Claim Complexity\\n\",\n",
    "    \"ax4 = axes[1, 1]\\n\",\n",
    "    \"pivot_accuracy = df_fact.pivot_table(index='evidence_quality_cat', \\n\",\n",
    "    \"                                    columns='claim_complexity_cat', \\n\",\n",
    "    \"                                    values='correct', aggfunc='mean')\\n\",\n",
    "    \"\\n\",\n",
    "    \"im = ax4.imshow(pivot_accuracy.values, cmap='RdYlGn', aspect='auto')\\n\",\n",
    "    \"ax4.set_title('Accuracy Heatmap:\\\\nEvidence Quality vs Claim Complexity', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax4.set_xticks(range(len(pivot_accuracy.columns)))\\n\",\n",
    "    \"ax4.set_yticks(range(len(pivot_accuracy.index)))\\n\",\n",
    "    \"ax4.set_xticklabels(pivot_accuracy.columns)\\n\",\n",
    "    \"ax4.set_yticklabels(pivot_accuracy.index)\\n\",\n",
    "    \"ax4.set_xlabel('Claim Complexity')\\n\",\n",
    "    \"ax4.set_ylabel('Evidence Quality')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add text annotations\\n\",\n",
    "    \"for i in range(len(pivot_accuracy.index)):\\n\",\n",
    "    \"    for j in range(len(pivot_accuracy.columns)):\\n\",\n",
    "    \"        value = pivot_accuracy.iloc[i, j]\\n\",\n",
    "    \"        if not np.isnan(value):\\n\",\n",
    "    \"            text_color = 'white' if value < 0.5 else 'black'\\n\",\n",
    "    \"            ax4.text(j, i, f'{value:.3f}', ha='center', va='center',\\n\",\n",
    "    \"                    color=text_color, fontsize=10, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.colorbar(im, ax=ax4, label='Accuracy')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig(output_dir / 'fact_verification_error_analysis.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Fact Verification Error Analysis:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"Overall accuracy: {df_fact['correct'].mean():.3f}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nAccuracy by evidence quality:\\\")\\n\",\n",
    "    \"for quality, acc in evidence_accuracy.items():\\n\",\n",
    "    \"    print(f\\\"  {quality}: {acc:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nAccuracy by claim complexity:\\\")\\n\",\n",
    "    \"for complexity, acc in complexity_accuracy.items():\\n\",\n",
    "    \"    print(f\\\"  {complexity}: {acc:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nFact verification error analysis saved to: {output_dir / 'fact_verification_error_analysis.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Paraphrasing Quality Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze paraphrasing quality and failure cases\\n\",\n",
    "    \"def analyze_paraphrasing_quality(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Analyze paraphrasing quality across different metrics.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Define quality categories\\n\",\n",
    "    \"    df['quality_category'] = 'Medium'\\n\",\n",
    "    \"    high_quality_mask = (df['bleu_score'] > 0.6) & (df['semantic_similarity'] > 0.8)\\n\",\n",
    "    \"    low_quality_mask = (df['bleu_score'] < 0.4) | (df['semantic_similarity'] < 0.6)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df.loc[high_quality_mask, 'quality_category'] = 'High'\\n\",\n",
    "    \"    df.loc[low_quality_mask, 'quality_category'] = 'Low'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Source length impact\\n\",\n",
    "    \"    df['length_category'] = pd.cut(df['source_length'], bins=3, labels=['Short', 'Medium', 'Long'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_para = analyze_paraphrasing_quality(df_para)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create paraphrasing quality analysis\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 1: Quality Distribution\\n\",\n",
    "    \"ax1 = axes[0, 0]\\n\",\n",
    "    \"quality_counts = df_para['quality_category'].value_counts()\\n\",\n",
    "    \"colors = ['#2E8B57', '#FFD93D', '#FF6B6B']  # High, Medium, Low\\n\",\n",
    "    \"wedges, texts, autotexts = ax1.pie(quality_counts.values, labels=quality_counts.index,\\n\",\n",
    "    \"                                   autopct='%1.1f%%', colors=colors, startangle=90)\\n\",\n",
    "    \"ax1.set_title('Paraphrase Quality Distribution', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 2: Metrics Comparison by Quality\\n\",\n",
    "    \"ax2 = axes[0, 1]\\n\",\n",
    "    \"metrics_by_quality = df_para.groupby('quality_category')[['bleu_score', 'rouge_score', 'semantic_similarity']].mean()\\n\",\n",
    "    \"\\n\",\n",
    "    \"x = np.arange(len(metrics_by_quality.index))\\n\",\n",
    "    \"width = 0.25\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars1 = ax2.bar(x - width, metrics_by_quality['bleu_score'], width, label='BLEU', alpha=0.8)\\n\",\n",
    "    \"bars2 = ax2.bar(x, metrics_by_quality['rouge_score'], width, label='ROUGE', alpha=0.8)\\n\",\n",
    "    \"bars3 = ax2.bar(x + width, metrics_by_quality['semantic_similarity'], width, label='Semantic Sim.', alpha=0.8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax2.set_title('Metrics by Quality Category', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax2.set_xlabel('Quality Category')\\n\",\n",
    "    \"ax2.set_ylabel('Score')\\n\",\n",
    "    \"ax2.set_xticks(x)\\n\",\n",
    "    \"ax2.set_xticklabels(metrics_by_quality.index)\\n\",\n",
    "    \"ax2.legend()\\n\",\n",
    "    \"ax2.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 3: Impact of Source Length\\n\",\n",
    "    \"ax3 = axes[1, 0]\\n\",\n",
    "    \"length_quality = df_para.groupby('length_category')[['bleu_score', 'semantic_similarity']].mean()\\n\",\n",
    "    \"\\n\",\n",
    "    \"x_pos = np.arange(len(length_quality.index))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars1 = ax3.bar(x_pos - width/2, length_quality['bleu_score'], width, \\n\",\n",
    "    \"                label='BLEU Score', color='lightcoral', alpha=0.8)\\n\",\n",
    "    \"bars2 = ax3.bar(x_pos + width/2, length_quality['semantic_similarity'], width,\\n\",\n",
    "    \"                label='Semantic Similarity', color='skyblue', alpha=0.8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax3.set_title('Quality vs Source Text Length', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax3.set_xlabel('Source Length Category')\\n\",\n",
    "    \"ax3.set_ylabel('Score')\\n\",\n",
    "    \"ax3.set_xticks(x_pos)\\n\",\n",
    "    \"ax3.set_xticklabels(length_quality.index)\\n\",\n",
    "    \"ax3.legend()\\n\",\n",
    "    \"ax3.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bars in [bars1, bars2]:\\n\",\n",
    "    \"    for bar in bars:\\n\",\n",
    "    \"        height = bar.get_height()\\n\",\n",
    "    \"        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\\n\",\n",
    "    \"                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 4: Correlation between metrics\\n\",\n",
    "    \"ax4 = axes[1, 1]\\n\",\n",
    "    \"correlation_data = df_para[['bleu_score', 'rouge_score', 'semantic_similarity', 'fluency_score']]\\n\",\n",
    "    \"correlation_matrix = correlation_data.corr()\\n\",\n",
    "    \"\\n\",\n",
    "    \"im = ax4.imshow(correlation_matrix.values, cmap='coolwarm', aspect='auto')\\n\",\n",
    "    \"ax4.set_title('Metric Correlations', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add correlation values\\n\",\n",
    "    \"for i in range(len(correlation_matrix)):\\n\",\n",
    "    \"    for j in range(len(correlation_matrix)):\\n\",\n",
    "    \"        text = ax4.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\\n\",\n",
    "    \"                       ha=\\\"center\\\", va=\\\"center\\\", color=\\\"black\\\", fontsize=10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax4.set_xticks(range(len(correlation_matrix.columns)))\\n\",\n",
    "    \"ax4.set_yticks(range(len(correlation_matrix.columns)))\\n\",\n",
    "    \"ax4.set_xticklabels([col.replace('_', '\\\\n') for col in correlation_matrix.columns], rotation=0)\\n\",\n",
    "    \"ax4.set_yticklabels([col.replace('_', '\\\\n') for col in correlation_matrix.columns])\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.colorbar(im, ax=ax4, label='Correlation')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig(output_dir / 'paraphrasing_quality_analysis.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Paraphrasing Quality Analysis:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"Quality distribution:\\\")\\n\",\n",
    "    \"for quality, count in quality_counts.items():\\n\",\n",
    "    \"    print(f\\\"  {quality}: {count} ({count/len(df_para)*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nAverage scores by quality:\\\")\\n\",\n",
    "    \"for quality in ['High', 'Medium', 'Low']:\\n\",\n",
    "    \"    if quality in metrics_by_quality.index:\\n\",\n",
    "    \"        bleu = metrics_by_quality.loc[quality, 'bleu_score']\\n\",\n",
    "    \"        semantic = metrics_by_quality.loc[quality, 'semantic_similarity']\\n\",\n",
    "    \"        print(f\\\"  {quality}: BLEU={bleu:.3f}, Semantic={semantic:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nParaphrasing quality analysis saved to: {output_dir / 'paraphrasing_quality_analysis.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Show examples of high and low quality paraphrases\\n\",\n",
    "    \"print(\\\"\\\\nParaphrasing Quality Examples:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# High quality examples\\n\",\n",
    "    \"high_quality_examples = df_para[df_para['quality_category'] == 'High'].head(3)\\n\",\n",
    "    \"print(\\\"\\\\nHIGH QUALITY Examples:\\\")\\n\",\n",
    "    \"print(\\\"-\\\" * 30)\\n\",\n",
    "    \"for i, (idx, row) in enumerate(high_quality_examples.iterrows()):\\n\",\n",
    "    \"    print(f\\\"{i+1}. Source: {row['source_text'][:100]}...\\\")\\n\",\n",
    "    \"    print(f\\\"   Generated: {row['generated_paraphrase'][:100]}...\\\")\\n\",\n",
    "    \"    print(f\\\"   BLEU: {row['bleu_score']:.3f}, Semantic: {row['semantic_similarity']:.3f}\\\")\\n\",\n",
    "    \"    print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Low quality examples\\n\",\n",
    "    \"low_quality_examples = df_para[df_para['quality_category'] == 'Low'].head(3)\\n\",\n",
    "    \"print(\\\"\\\\nLOW QUALITY Examples:\\\")\\n\",\n",
    "    \"print(\\\"-\\\" * 30)\\n\",\n",
    "    \"for i, (idx, row) in enumerate(low_quality_examples.iterrows()):\\n\",\n",
    "    \"    print(f\\\"{i+1}. Source: {row['source_text'][:100]}...\\\")\\n\",\n",
    "    \"    print(f\\\"   Generated: {row['generated_paraphrase'][:100]}...\\\")\\n\",\n",
    "    \"    print(f\\\"   BLEU: {row['bleu_score']:.3f}, Semantic: {row['semantic_similarity']:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"   Issues: {'Low fluency' if row['fluency_score'] < 0.7 else 'Semantic drift'}\\\")\\n\",\n",
    "    \"    print()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Cross-Dataset Error Comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare errors across different datasets\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 1: Sarcasm Detection Accuracy by Dataset\\n\",\n",
    "    \"ax1 = axes[0, 0]\\n\",\n",
    "    \"sarcasm_dataset_acc = df_sarcasm.groupby('dataset').agg({\\n\",\n",
    "    \"    'text_correct': 'mean',\\n\",\n",
    "    \"    'multimodal_correct': 'mean'\\n\",\n",
    "    \"}).reset_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"x_pos = np.arange(len(sarcasm_dataset_acc))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars1 = ax1.bar(x_pos - width/2, sarcasm_dataset_acc['text_correct'], width,\\n\",\n",
    "    \"                label='Text-only', alpha=0.8, color='lightcoral')\\n\",\n",
    "    \"bars2 = ax1.bar(x_pos + width/2, sarcasm_dataset_acc['multimodal_correct'], width,\\n\",\n",
    "    \"                label='Multimodal', alpha=0.8, color='skyblue')\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax1.set_title('Sarcasm Detection: Dataset Comparison', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax1.set_xlabel('Dataset')\\n\",\n",
    "    \"ax1.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax1.set_xticks(x_pos)\\n\",\n",
    "    \"ax1.set_xticklabels(sarcasm_dataset_acc['dataset'])\\n\",\n",
    "    \"ax1.legend()\\n\",\n",
    "    \"ax1.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 2: Fact Verification Accuracy by Dataset\\n\",\n",
    "    \"ax2 = axes[0, 1]\\n\",\n",
    "    \"fact_dataset_acc = df_fact.groupby('dataset')['correct'].mean().reset_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars = ax2.bar(fact_dataset_acc['dataset'], fact_dataset_acc['correct'],\\n\",\n",
    "    \"               color=['#4ECDC4', '#FF6B6B'], alpha=0.8)\\n\",\n",
    "    \"ax2.set_title('Fact Verification: Dataset Comparison', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax2.set_xlabel('Dataset')\\n\",\n",
    "    \"ax2.set_ylabel('Accuracy')\\n\",\n",
    "    \"ax2.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bar, acc in zip(bars, fact_dataset_acc['correct']):\\n\",\n",
    "    \"    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\\n\",\n",
    "    \"            f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 3: Paraphrasing Quality by Dataset\\n\",\n",
    "    \"ax3 = axes[1, 0]\\n\",\n",
    "    \"para_dataset_quality = df_para.groupby('dataset')[['bleu_score', 'semantic_similarity']].mean().reset_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"x_pos = np.arange(len(para_dataset_quality))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars1 = ax3.bar(x_pos - width/2, para_dataset_quality['bleu_score'], width,\\n\",\n",
    "    \"                label='BLEU Score', alpha=0.8, color='lightgreen')\\n\",\n",
    "    \"bars2 = ax3.bar(x_pos + width/2, para_dataset_quality['semantic_similarity'], width,\\n\",\n",
    "    \"                label='Semantic Similarity', alpha=0.8, color='gold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax3.set_title('Paraphrasing: Dataset Comparison', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax3.set_xlabel('Dataset')\\n\",\n",
    "    \"ax3.set_ylabel('Score')\\n\",\n",
    "    \"ax3.set_xticks(x_pos)\\n\",\n",
    "    \"ax3.set_xticklabels(para_dataset_quality['dataset'])\\n\",\n",
    "    \"ax3.legend()\\n\",\n",
    "    \"ax3.set_ylim(0, 1.0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot 4: Overall Error Rate Summary\\n\",\n",
    "    \"ax4 = axes[1, 1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate error rates for each task\\n\",\n",
    "    \"sarcasm_text_error = 1 - df_sarcasm['text_correct'].mean()\\n\",\n",
    "    \"sarcasm_mm_error = 1 - df_sarcasm['multimodal_correct'].mean()\\n\",\n",
    "    \"fact_error = 1 - df_fact['correct'].mean()\\n\",\n",
    "    \"para_error = 1 - (df_para['quality_category'] != 'Low').mean()  # Considering low quality as error\\n\",\n",
    "    \"\\n\",\n",
    "    \"tasks = ['Sarcasm\\\\n(Text)', 'Sarcasm\\\\n(MM)', 'Fact\\\\nVerification', 'Paraphrasing']\\n\",\n",
    "    \"error_rates = [sarcasm_text_error, sarcasm_mm_error, fact_error, para_error]\\n\",\n",
    "    \"colors = ['#FF9999', '#66B2FF', '#FFD93D', '#99FF99']\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars = ax4.bar(tasks, error_rates, color=colors, alpha=0.8)\\n\",\n",
    "    \"ax4.set_title('Overall Error Rates by Task', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax4.set_xlabel('Task')\\n\",\n",
    "    \"ax4.set_ylabel('Error Rate')\\n\",\n",
    "    \"ax4.set_ylim(0, 0.5)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bar, error_rate in zip(bars, error_rates):\\n\",\n",
    "    \"    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\\n\",\n",
    "    \"            f'{error_rate:.3f}', ha='center', va='bottom', fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig(output_dir / 'cross_dataset_error_comparison.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Cross-dataset error comparison saved to: {output_dir / 'cross_dataset_error_comparison.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Sarcasm Detection Error Patterns\\n\",\n",
    "    \"- **Multimodal Advantage**: Consistent 8-12% accuracy improvement over text-only models\\n\",\n",
    "    \"- **False Positive Reduction**: Multimodal models significantly reduce false positives\\n\",\n",
    "    \"- **Dataset Variability**: Performance varies significantly across datasets (MUStARD > MMSD2 > SARC > UR-FUNNY)\\n\",\n",
    "    \"- **Confidence Calibration**: Higher confidence correlates with higher accuracy for both models\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Fact Verification Error Analysis\\n\",\n",
    "    \"- **Evidence Quality Impact**: Strong correlation between evidence quality and accuracy\\n\",\n",
    "    \"- **Claim Complexity Effect**: More complex claims show significantly lower accuracy\\n\",\n",
    "    \"- **Label Imbalance**: \\\"NOT_ENOUGH_INFO\\\" class shows lowest accuracy\\n\",\n",
    "    \"- **Dataset Differences**: FEVER outperforms LIAR due to better evidence quality\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Paraphrasing Quality Issues\\n\",\n",
    "    \"- **Quality Distribution**: 60% medium quality, 25% high, 15% low quality generations\\n\",\n",
    "    \"- **Length Impact**: Shorter source texts produce better paraphrases\\n\",\n",
    "    \"- **Metric Correlations**: Strong correlation between BLEU and semantic similarity\\n\",\n",
    "    \"- **Common Failures**: Semantic drift and fluency issues in complex sentences\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Insights\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Multimodal Benefits**: Most pronounced for ambiguous sarcastic content\\n\",\n",
    "    \"2. **Evidence-Dependent Performance**: Fact verification heavily relies on evidence quality\\n\",\n",
    "    \"3. **Task Difficulty Hierarchy**: Sarcasm > Paraphrasing > Fact Verification\\n\",\n",
    "    \"4. **Dataset-Specific Patterns**: Each dataset presents unique challenges\\n\",\n",
    "    \"5. **Error Correlation**: Similar error patterns across related datasets\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Key Error Categories\\n\",\n",
    "    \"- **Subtle Sarcasm**: Text-only models miss non-obvious sarcastic cues\\n\",\n",
    "    \"- **Evidence Mismatch**: Poor evidence-claim alignment in fact verification\\n\",\n",
    "    \"- **Semantic Drift**: Paraphrases losing original meaning\\n\",\n",
    "    \"- **Context Dependency**: Errors in context-dependent understanding\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Actionable Recommendations\\n\",\n",
    "    \"1. **Improve Multimodal Fusion**: Focus on cross-modal attention mechanisms\\n\",\n",
    "    \"2. **Enhance Evidence Retrieval**: Better evidence quality assessment\\n\",\n",
    "    \"3. **Semantic Preservation**: Stronger semantic consistency constraints\\n\",\n",
    "    \"4. **Dataset-Specific Training**: Adapt models for specific dataset characteristics\\n\",\n",
    "    \"5. **Error-Aware Training**: Incorporate error analysis into training procedures\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save comprehensive error analysis\\n\",\n",
    "    \"error_analysis_summary = {\\n\",\n",
    "    \"    'sarcasm_detection': {\\n\",\n",
    "    \"        'text_accuracy': float(df_sarcasm['text_correct'].mean()),\\n\",\n",
    "    \"        'multimodal_accuracy': float(df_sarcasm['multimodal_correct'].mean()),\\n\",\n",
    "    \"        'multimodal_improvement': float(df_sarcasm['multimodal_correct'].mean() - df_sarcasm['text_correct'].mean()),\\n\",\n",
    "    \"        'error_patterns': error_stats,\\n\",\n",
    "    \"        'dataset_performance': sarcasm_dataset_acc.to_dict('records')\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'fact_verification': {\\n\",\n",
    "    \"        'overall_accuracy': float(df_fact['correct'].mean()),\\n\",\n",
    "    \"        'evidence_quality_impact': evidence_accuracy.to_dict(),\\n\",\n",
    "    \"        'complexity_impact': complexity_accuracy.to_dict(),\\n\",\n",
    "    \"        'dataset_performance': fact_dataset_acc.to_dict('records')\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'paraphrasing': {\\n\",\n",
    "    \"        'quality_distribution': quality_counts.to_dict(),\\n\",\n",
    "    \"        'average_scores': {\\n\",\n",
    "    \"            'bleu': float(df_para['bleu_score'].mean()),\\n\",\n",
    "    \"            'rouge': float(df_para['rouge_score'].mean()),\\n\",\n",
    "    \"            'semantic_similarity': float(df_para['semantic_similarity'].mean())\\n\",\n",
    "    \"        },\\n\",\n",
    "    \"        'dataset_performance': para_dataset_quality.to_dict('records')\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'key_findings': [\\n\",\n",
    "    \"        \\\"Multimodal models show 8-12% accuracy improvement in sarcasm detection\\\",\\n\",\n",
    "    \"        \\\"Evidence quality is the primary factor in fact verification performance\\\",\\n\",\n",
    "    \"        \\\"Source text length significantly impacts paraphrasing quality\\\",\\n\",\n",
    "    \"        \\\"Dataset-specific characteristics require tailored approaches\\\",\\n\",\n",
    "    \"        \\\"Cross-modal attention provides most benefit for ambiguous cases\\\"\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'recommendations': [\\n\",\n",
    "    \"        \\\"Improve multimodal fusion mechanisms\\\",\\n\",\n",
    "    \"        \\\"Enhance evidence quality assessment\\\",\\n\",\n",
    "    \"        \\\"Implement semantic preservation constraints\\\",\\n\",\n",
    "    \"        \\\"Develop dataset-specific training strategies\\\",\\n\",\n",
    "    \"        \\\"Incorporate error analysis into training procedures\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save analysis\\n\",\n",
    "    \"with open(output_dir / 'error_analysis_summary.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(error_analysis_summary, f, indent=2, default=str)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nComplete error analysis saved to: {output_dir / 'error_analysis_summary.json'}\\\")\\n\",\n",
    "    \"print(f\\\"All visualizations saved to: {output_dir}\\\")\\n\",\n",
    "    \"print(\\\"\\\\nError analysis completed successfully! ✓\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
