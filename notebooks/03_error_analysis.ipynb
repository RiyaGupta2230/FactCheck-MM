{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d26945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock error data for fact verification\n",
    "fv_datasets = ['fever', 'liar']\n",
    "fv_error_types = ['retrieval_failure', 'classification_error', 'evidence_noise', 'label_ambiguity']\n",
    "\n",
    "fv_errors = {}\n",
    "for dataset in fv_datasets:\n",
    "    fv_errors[dataset] = {\n",
    "        'retrieval_failure': np.random.randint(8, 28),\n",
    "        'classification_error': np.random.randint(10, 35),\n",
    "        'evidence_noise': np.random.randint(5, 20),\n",
    "        'label_ambiguity': np.random.randint(3, 15)\n",
    "    }\n",
    "\n",
    "fv_error_df = pd.DataFrame(fv_errors).T\n",
    "print(\"\\nFact Verification Error Distribution:\")\n",
    "print(\"=\"*80)\n",
    "print(fv_error_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_fv_errors = fv_error_df.sum().sum()\n",
    "print(f\"\\nTotal Verification Errors Analyzed: {total_fv_errors}\")\n",
    "for error_type in fv_error_types:\n",
    "    count = fv_error_df[error_type].sum()\n",
    "    pct = (count / total_fv_errors * 100) if total_fv_errors > 0 else 0\n",
    "    print(f\"  {error_type}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nError analysis completed successfully! âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91e061",
   "metadata": {},
   "source": [
    "## Fact Verification Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock error data for paraphrasing\n",
    "paraphrase_datasets = ['paranmt', 'mrpc', 'quora']\n",
    "paraphrase_error_types = ['low_quality', 'semantic_divergence', 'length_error', 'token_repetition']\n",
    "\n",
    "paraphrase_errors = {}\n",
    "for dataset in paraphrase_datasets:\n",
    "    paraphrase_errors[dataset] = {\n",
    "        'low_quality': np.random.randint(10, 40),\n",
    "        'semantic_divergence': np.random.randint(8, 25),\n",
    "        'length_error': np.random.randint(3, 15),\n",
    "        'token_repetition': np.random.randint(2, 12)\n",
    "    }\n",
    "\n",
    "paraphrase_error_df = pd.DataFrame(paraphrase_errors).T\n",
    "print(\"\\nParaphrasing Error Distribution:\")\n",
    "print(\"=\"*80)\n",
    "print(paraphrase_error_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_p_errors = paraphrase_error_df.sum().sum()\n",
    "print(f\"\\nTotal Generation Errors Analyzed: {total_p_errors}\")\n",
    "for error_type in paraphrase_error_types:\n",
    "    count = paraphrase_error_df[error_type].sum()\n",
    "    pct = (count / total_p_errors * 100) if total_p_errors > 0 else 0\n",
    "    print(f\"  {error_type}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a364fa",
   "metadata": {},
   "source": [
    "## Paraphrasing Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock error data for sarcasm detection\n",
    "sarcasm_datasets = ['sarc', 'mmsd2', 'mustard', 'sarcnet', 'sarcasm_headlines']\n",
    "error_types = ['false_positive', 'false_negative', 'context_confusion', 'modality_mismatch']\n",
    "\n",
    "sarcasm_errors = {}\n",
    "for dataset in sarcasm_datasets:\n",
    "    sarcasm_errors[dataset] = {\n",
    "        'false_positive': np.random.randint(5, 25),\n",
    "        'false_negative': np.random.randint(8, 30),\n",
    "        'context_confusion': np.random.randint(3, 15),\n",
    "        'modality_mismatch': np.random.randint(2, 10) if dataset in ['mmsd2', 'mustard', 'sarcnet'] else 0\n",
    "    }\n",
    "\n",
    "sarcasm_error_df = pd.DataFrame(sarcasm_errors).T\n",
    "print(\"\\nSarcasm Detection Error Distribution:\")\n",
    "print(\"=\"*80)\n",
    "print(sarcasm_error_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Error percentages\n",
    "total_errors = sarcasm_error_df.sum().sum()\n",
    "print(f\"\\nTotal Classification Errors Analyzed: {total_errors}\")\n",
    "for error_type in error_types:\n",
    "    count = sarcasm_error_df[error_type].sum()\n",
    "    pct = (count / total_errors * 100) if total_errors > 0 else 0\n",
    "    print(f\"  {error_type}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ad0c6",
   "metadata": {},
   "source": [
    "## Sarcasm Detection Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68887a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().cwd().parent if Path().cwd().name == 'notebooks' else Path().cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / 'outputs' / 'error_analysis'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98098819",
   "metadata": {},
   "source": [
    "# FactCheck-MM Error Analysis\n",
    "\n",
    "## Overview\n",
    "Comprehensive error analysis for FactCheck-MM across canonical datasets:\n",
    "\n",
    "**Sarcasm Detection** (5 datasets):\n",
    "- sarc, mmsd2, mustard, sarcnet, sarcasm_headlines\n",
    "\n",
    "**Paraphrasing** (3 datasets):\n",
    "- paranmt, mrpc, quora\n",
    "\n",
    "**Fact Verification** (2 datasets):\n",
    "- fever, liar\n",
    "\n",
    "## Analysis Focus\n",
    "- Misclassification patterns\n",
    "- Multimodal vs text-only error comparison\n",
    "- Task-specific failure cases\n",
    "- Dataset-specific error patterns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
