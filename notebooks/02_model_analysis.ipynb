{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1395cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact verification models and datasets\n",
    "fv_models = {\n",
    "    'bert_fc': {'name': 'BERT + FC', 'params': '110M'},\n",
    "    'rte_finetuned': {'name': 'RTE-Finetuned', 'params': '340M'},\n",
    "    'xlnet_large': {'name': 'XLNet-Large', 'params': '340M'}\n",
    "}\n",
    "\n",
    "fv_datasets = ['fever', 'liar']\n",
    "\n",
    "# Example metrics (Accuracy)\n",
    "fv_performance = {\n",
    "    'bert_fc': {'fever': 0.712, 'liar': 0.681},\n",
    "    'rte_finetuned': {'fever': 0.745, 'liar': 0.718},\n",
    "    'xlnet_large': {'fever': 0.758, 'liar': 0.729}\n",
    "}\n",
    "\n",
    "fv_perf_df = pd.DataFrame(fv_performance).T\n",
    "print(\"\\nFact Verification Performance (Accuracy):\")\n",
    "print(\"=\"*80)\n",
    "print(fv_perf_df.to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nModel analysis completed successfully! âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca95ce",
   "metadata": {},
   "source": [
    "## Fact Verification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b165f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphrasing models and datasets\n",
    "paraphrase_models = {\n",
    "    'bert2bert': {'name': 'BERT2BERT', 'params': '220M'},\n",
    "    'transformer_small': {'name': 'Transformer-Small', 'params': '90M'},\n",
    "    'transformer_base': {'name': 'Transformer-Base', 'params': '230M'},\n",
    "}\n",
    "\n",
    "paraphrase_datasets = ['paranmt', 'mrpc', 'quora']\n",
    "\n",
    "# Example metrics (BLEU, METEOR, BERTScore)\n",
    "paraphrase_performance = {\n",
    "    'bert2bert': {'paranmt': 38.2, 'mrpc': 52.1, 'quora': 45.8},\n",
    "    'transformer_small': {'paranmt': 36.5, 'mrpc': 50.3, 'quora': 44.1},\n",
    "    'transformer_base': {'paranmt': 40.1, 'mrpc': 54.7, 'quora': 47.9}\n",
    "}\n",
    "\n",
    "paraphrase_perf_df = pd.DataFrame(paraphrase_performance).T\n",
    "print(\"\\nParaphrasing Performance (BLEU Scores):\")\n",
    "print(\"=\"*80)\n",
    "print(paraphrase_perf_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b0a98",
   "metadata": {},
   "source": [
    "## Paraphrasing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarcasm detection models and datasets\n",
    "sarcasm_models = {\n",
    "    'bert_base': {'name': 'BERT-Base', 'modality': 'text', 'params': '110M'},\n",
    "    'bert_large': {'name': 'BERT-Large', 'modality': 'text', 'params': '340M'},\n",
    "    'roberta_base': {'name': 'RoBERTa-Base', 'modality': 'text', 'params': '125M'},\n",
    "    'multimodal_fusion': {'name': 'Multimodal Fusion', 'modality': 'text+image+audio+video', 'params': '200M'}\n",
    "}\n",
    "\n",
    "sarcasm_datasets = ['sarc', 'mmsd2', 'mustard', 'sarcnet', 'sarcasm_headlines']\n",
    "\n",
    "# Example performance metrics (template for real metrics)\n",
    "sarcasm_performance = {\n",
    "    'bert_base': {'sarc': 0.845, 'mmsd2': 0.782, 'mustard': 0.756, 'sarcnet': 0.712, 'sarcasm_headlines': 0.823},\n",
    "    'bert_large': {'sarc': 0.861, 'mmsd2': 0.798, 'mustard': 0.768, 'sarcnet': 0.728, 'sarcasm_headlines': 0.839},\n",
    "    'roberta_base': {'sarc': 0.859, 'mmsd2': 0.805, 'mustard': 0.771, 'sarcnet': 0.731, 'sarcasm_headlines': 0.841},\n",
    "    'multimodal_fusion': {'sarc': 0.856, 'mmsd2': 0.832, 'mustard': 0.814, 'sarcnet': 0.789, 'sarcasm_headlines': 0.847}\n",
    "}\n",
    "\n",
    "# Create performance dataframe\n",
    "sarcasm_perf_df = pd.DataFrame(sarcasm_performance).T\n",
    "print(\"\\nSarcasm Detection Performance (F1 Scores):\")\n",
    "print(\"=\"*80)\n",
    "print(sarcasm_perf_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a95db",
   "metadata": {},
   "source": [
    "## Sarcasm Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().cwd().parent if Path().cwd().name == 'notebooks' else Path().cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = project_root / 'outputs' / 'model_analysis'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77425381",
   "metadata": {},
   "source": [
    "# FactCheck-MM Model Analysis\n",
    "\n",
    "## Overview\n",
    "Comprehensive analysis of FactCheck-MM model performance across three tasks:\n",
    "- **Sarcasm Detection** (sarc, mmsd2, mustard, sarcnet, sarcasm_headlines)\n",
    "- **Paraphrasing** (paranmt, mrpc, quora)\n",
    "- **Fact Verification** (fever, liar)\n",
    "\n",
    "## Analysis Scope\n",
    "- Training curve analysis across datasets\n",
    "- Model architecture comparison (text vs multimodal)\n",
    "- Performance metrics evaluation\n",
    "- Cross-dataset and cross-task analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
