{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c84c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# FactCheck-MM Advanced Visualizations\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Overview\\n\",\n",
    "    \"This notebook provides advanced visualization capabilities for FactCheck-MM models including:\\n\",\n",
    "    \"- Cross-modal attention heatmaps\\n\",\n",
    "    \"- Grad-CAM visualizations for vision encoders\\n\",\n",
    "    \"- Audio attention maps for speech analysis\\n\",\n",
    "    \"- Interactive embeddings visualization\\n\",\n",
    "    \"- Publication-ready plots and figures\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Method\\n\",\n",
    "    \"We implement specialized visualization techniques:\\n\",\n",
    "    \"- **Attention Visualization**: Cross-modal attention patterns\\n\",\n",
    "    \"- **Saliency Maps**: Grad-CAM for visual interpretability\\n\",\n",
    "    \"- **Embedding Projections**: t-SNE/UMAP for high-dimensional data\\n\",\n",
    "    \"- **Interactive Plots**: Plotly-based interactive visualizations\\n\",\n",
    "    \"\\n\",\n",
    "    \"All visualizations are designed for both analysis and publication quality.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Setup and imports\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"import plotly.figure_factory as ff\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add project root to path\\n\",\n",
    "    \"project_root = Path().cwd().parent if Path().cwd().name == 'notebooks' else Path().cwd()\\n\",\n",
    "    \"sys.path.insert(0, str(project_root))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project utilities\\n\",\n",
    "    \"from shared.utils.visualization import create_attention_heatmap, create_embedding_plot\\n\",\n",
    "    \"from shared.utils.interpretability import GradCAMVisualizer, AttentionVisualizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Optional imports for advanced visualizations\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    import umap\\n\",\n",
    "    \"    UMAP_AVAILABLE = True\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    UMAP_AVAILABLE = False\\n\",\n",
    "    \"    print(\\\"UMAP not available. Install with: pip install umap-learn\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from captum.attr import GradientShap, IntegratedGradients\\n\",\n",
    "    \"    CAPTUM_AVAILABLE = True\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    CAPTUM_AVAILABLE = False\\n\",\n",
    "    \"    print(\\\"Captum not available. Install with: pip install captum\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set style\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create output directories\\n\",\n",
    "    \"output_dir = project_root / 'outputs' / 'notebooks'\\n\",\n",
    "    \"docs_dir = project_root / 'docs' / 'figures'\\n\",\n",
    "    \"output_dir.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "    \"docs_dir.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Project root: {project_root}\\\")\\n\",\n",
    "    \"print(f\\\"Output directory: {output_dir}\\\")\\n\",\n",
    "    \"print(f\\\"Docs directory: {docs_dir}\\\")\\n\",\n",
    "    \"print(f\\\"UMAP available: {UMAP_AVAILABLE}\\\")\\n\",\n",
    "    \"print(f\\\"Captum available: {CAPTUM_AVAILABLE}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Cross-Modal Attention Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create cross-modal attention heatmaps\\n\",\n",
    "    \"def create_cross_modal_attention_viz():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create comprehensive cross-modal attention visualizations.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Mock attention data for demonstration\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Text tokens\\n\",\n",
    "    \"    text_tokens = [\\\"Oh\\\", \\\"great\\\", \\\",\\\", \\\"another\\\", \\\"Monday\\\", \\\"morning\\\", \\\"meeting\\\", \\\"[CLS]\\\", \\\"[SEP]\\\"]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Audio frame indices\\n\",\n",
    "    \"    audio_frames = [f\\\"Audio_{i}\\\" for i in range(8)]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Image patch indices  \\n\",\n",
    "    \"    image_patches = [f\\\"Patch_{i}\\\" for i in range(12)]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate mock attention matrices\\n\",\n",
    "    \"    text_audio_attention = np.random.rand(len(text_tokens), len(audio_frames))\\n\",\n",
    "    \"    text_image_attention = np.random.rand(len(text_tokens), len(image_patches))\\n\",\n",
    "    \"    audio_image_attention = np.random.rand(len(audio_frames), len(image_patches))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Normalize attention weights\\n\",\n",
    "    \"    text_audio_attention = F.softmax(torch.tensor(text_audio_attention), dim=1).numpy()\\n\",\n",
    "    \"    text_image_attention = F.softmax(torch.tensor(text_image_attention), dim=1).numpy()\\n\",\n",
    "    \"    audio_image_attention = F.softmax(torch.tensor(audio_image_attention), dim=1).numpy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create attention heatmaps\\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 1: Text-Audio Attention\\n\",\n",
    "    \"    ax1 = axes[0, 0]\\n\",\n",
    "    \"    im1 = ax1.imshow(text_audio_attention, cmap='Blues', aspect='auto')\\n\",\n",
    "    \"    ax1.set_title('Text-Audio Cross-Modal Attention', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax1.set_xlabel('Audio Frames')\\n\",\n",
    "    \"    ax1.set_ylabel('Text Tokens')\\n\",\n",
    "    \"    ax1.set_xticks(range(len(audio_frames)))\\n\",\n",
    "    \"    ax1.set_yticks(range(len(text_tokens)))\\n\",\n",
    "    \"    ax1.set_xticklabels(audio_frames, rotation=45, ha='right')\\n\",\n",
    "    \"    ax1.set_yticklabels(text_tokens)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add attention values\\n\",\n",
    "    \"    for i in range(len(text_tokens)):\\n\",\n",
    "    \"        for j in range(len(audio_frames)):\\n\",\n",
    "    \"            if text_audio_attention[i, j] > 0.1:  # Only show high attention\\n\",\n",
    "    \"                text = ax1.text(j, i, f'{text_audio_attention[i, j]:.2f}',\\n\",\n",
    "    \"                               ha=\\\"center\\\", va=\\\"center\\\", color=\\\"white\\\", fontsize=8)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.colorbar(im1, ax=ax1, label='Attention Weight')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 2: Text-Image Attention\\n\",\n",
    "    \"    ax2 = axes[0, 1]\\n\",\n",
    "    \"    im2 = ax2.imshow(text_image_attention, cmap='Reds', aspect='auto')\\n\",\n",
    "    \"    ax2.set_title('Text-Image Cross-Modal Attention', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax2.set_xlabel('Image Patches')\\n\",\n",
    "    \"    ax2.set_ylabel('Text Tokens')\\n\",\n",
    "    \"    ax2.set_xticks(range(len(image_patches)))\\n\",\n",
    "    \"    ax2.set_yticks(range(len(text_tokens)))\\n\",\n",
    "    \"    ax2.set_xticklabels(image_patches, rotation=45, ha='right')\\n\",\n",
    "    \"    ax2.set_yticklabels(text_tokens)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add attention values\\n\",\n",
    "    \"    for i in range(len(text_tokens)):\\n\",\n",
    "    \"        for j in range(len(image_patches)):\\n\",\n",
    "    \"            if text_image_attention[i, j] > 0.08:\\n\",\n",
    "    \"                text = ax2.text(j, i, f'{text_image_attention[i, j]:.2f}',\\n\",\n",
    "    \"                               ha=\\\"center\\\", va=\\\"center\\\", color=\\\"white\\\", fontsize=7)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.colorbar(im2, ax=ax2, label='Attention Weight')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 3: Audio-Image Attention\\n\",\n",
    "    \"    ax3 = axes[1, 0]\\n\",\n",
    "    \"    im3 = ax3.imshow(audio_image_attention, cmap='Greens', aspect='auto')\\n\",\n",
    "    \"    ax3.set_title('Audio-Image Cross-Modal Attention', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax3.set_xlabel('Image Patches')\\n\",\n",
    "    \"    ax3.set_ylabel('Audio Frames')\\n\",\n",
    "    \"    ax3.set_xticks(range(len(image_patches)))\\n\",\n",
    "    \"    ax3.set_yticks(range(len(audio_frames)))\\n\",\n",
    "    \"    ax3.set_xticklabels(image_patches, rotation=45, ha='right')\\n\",\n",
    "    \"    ax3.set_yticklabels(audio_frames)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add attention values\\n\",\n",
    "    \"    for i in range(len(audio_frames)):\\n\",\n",
    "    \"        for j in range(len(image_patches)):\\n\",\n",
    "    \"            if audio_image_attention[i, j] > 0.1:\\n\",\n",
    "    \"                text = ax3.text(j, i, f'{audio_image_attention[i, j]:.2f}',\\n\",\n",
    "    \"                               ha=\\\"center\\\", va=\\\"center\\\", color=\\\"white\\\", fontsize=8)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.colorbar(im3, ax=ax3, label='Attention Weight')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 4: Attention Flow Diagram\\n\",\n",
    "    \"    ax4 = axes[1, 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create a simplified attention flow visualization\\n\",\n",
    "    \"    modalities = ['Text', 'Audio', 'Image']\\n\",\n",
    "    \"    attention_strengths = np.array([\\n\",\n",
    "    \"        [0, np.mean(text_audio_attention), np.mean(text_image_attention)],\\n\",\n",
    "    \"        [np.mean(text_audio_attention), 0, np.mean(audio_image_attention)],\\n\",\n",
    "    \"        [np.mean(text_image_attention), np.mean(audio_image_attention), 0]\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    im4 = ax4.imshow(attention_strengths, cmap='viridis', aspect='auto')\\n\",\n",
    "    \"    ax4.set_title('Cross-Modal Attention Summary', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax4.set_xticks(range(len(modalities)))\\n\",\n",
    "    \"    ax4.set_yticks(range(len(modalities)))\\n\",\n",
    "    \"    ax4.set_xticklabels(modalities)\\n\",\n",
    "    \"    ax4.set_yticklabels(modalities)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add attention strength values\\n\",\n",
    "    \"    for i in range(len(modalities)):\\n\",\n",
    "    \"        for j in range(len(modalities)):\\n\",\n",
    "    \"            if i != j:\\n\",\n",
    "    \"                text = ax4.text(j, i, f'{attention_strengths[i, j]:.3f}',\\n\",\n",
    "    \"                               ha=\\\"center\\\", va=\\\"center\\\", color=\\\"white\\\", \\n\",\n",
    "    \"                               fontsize=12, fontweight='bold')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.colorbar(im4, ax=ax4, label='Average Attention')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    return fig, {\\n\",\n",
    "    \"        'text_audio': text_audio_attention,\\n\",\n",
    "    \"        'text_image': text_image_attention, \\n\",\n",
    "    \"        'audio_image': audio_image_attention\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create attention visualizations\\n\",\n",
    "    \"fig, attention_data = create_cross_modal_attention_viz()\\n\",\n",
    "    \"plt.savefig(output_dir / 'cross_modal_attention_heatmaps.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.savefig(docs_dir / 'cross_modal_attention_heatmaps.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Cross-modal attention heatmaps saved to:\\\")\\n\",\n",
    "    \"print(f\\\"  Analysis: {output_dir / 'cross_modal_attention_heatmaps.png'}\\\")\\n\",\n",
    "    \"print(f\\\"  Documentation: {docs_dir / 'cross_modal_attention_heatmaps.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Grad-CAM Visualization for Vision Encoders\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create Grad-CAM visualizations for image understanding\\n\",\n",
    "    \"def create_gradcam_visualization():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create Grad-CAM visualizations for vision encoder interpretability.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Create mock Grad-CAM heatmaps\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Simulate different image scenarios\\n\",\n",
    "    \"    scenarios = [\\n\",\n",
    "    \"        \\\"Facial Expression (Sarcastic)\\\",\\n\",\n",
    "    \"        \\\"Gesture Analysis\\\", \\n\",\n",
    "    \"        \\\"Context Objects\\\",\\n\",\n",
    "    \"        \\\"Scene Understanding\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for idx, scenario in enumerate(scenarios):\\n\",\n",
    "    \"        # Create mock original image (grayscale for simplicity)\\n\",\n",
    "    \"        original_image = np.random.rand(224, 224)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create mock Grad-CAM heatmap\\n\",\n",
    "    \"        # Focus attention on different regions based on scenario\\n\",\n",
    "    \"        if idx == 0:  # Facial expression\\n\",\n",
    "    \"            center_y, center_x = 80, 112  # Upper center (face region)\\n\",\n",
    "    \"        elif idx == 1:  # Gesture\\n\",\n",
    "    \"            center_y, center_x = 150, 80   # Lower left (hand region)\\n\",\n",
    "    \"        elif idx == 2:  # Objects\\n\",\n",
    "    \"            center_y, center_x = 112, 180  # Center right (object region)\\n\",\n",
    "    \"        else:  # Scene\\n\",\n",
    "    \"            center_y, center_x = 180, 112  # Lower center (scene context)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create Gaussian-like attention map\\n\",\n",
    "    \"        y, x = np.ogrid[:224, :224]\\n\",\n",
    "    \"        mask = ((x - center_x) ** 2 + (y - center_y) ** 2) < 40**2\\n\",\n",
    "    \"        gradcam_heatmap = np.zeros((224, 224))\\n\",\n",
    "    \"        gradcam_heatmap[mask] = 1.0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add some noise and smooth the heatmap\\n\",\n",
    "    \"        from scipy import ndimage\\n\",\n",
    "    \"        gradcam_heatmap = ndimage.gaussian_filter(gradcam_heatmap, sigma=15)\\n\",\n",
    "    \"        gradcam_heatmap = gradcam_heatmap / gradcam_heatmap.max()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot original image\\n\",\n",
    "    \"        ax_orig = axes[0, idx]\\n\",\n",
    "    \"        ax_orig.imshow(original_image, cmap='gray', alpha=0.7)\\n\",\n",
    "    \"        ax_orig.set_title(f'{scenario}\\\\nOriginal Image', fontsize=12, fontweight='bold')\\n\",\n",
    "    \"        ax_orig.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add mock image content indicators\\n\",\n",
    "    \"        if idx == 0:  # Face\\n\",\n",
    "    \"            circle = plt.Circle((112, 80), 30, fill=False, color='yellow', linewidth=2)\\n\",\n",
    "    \"            ax_orig.add_patch(circle)\\n\",\n",
    "    \"            ax_orig.text(112, 50, 'Face', ha='center', color='yellow', fontweight='bold')\\n\",\n",
    "    \"        elif idx == 1:  # Gesture\\n\",\n",
    "    \"            rect = plt.Rectangle((65, 130), 30, 40, fill=False, color='cyan', linewidth=2)\\n\",\n",
    "    \"            ax_orig.add_patch(rect)\\n\",\n",
    "    \"            ax_orig.text(80, 190, 'Hand', ha='center', color='cyan', fontweight='bold')\\n\",\n",
    "    \"        elif idx == 2:  # Objects\\n\",\n",
    "    \"            rect = plt.Rectangle((160, 90), 40, 44, fill=False, color='lime', linewidth=2)\\n\",\n",
    "    \"            ax_orig.add_patch(rect)\\n\",\n",
    "    \"            ax_orig.text(180, 150, 'Object', ha='center', color='lime', fontweight='bold')\\n\",\n",
    "    \"        else:  # Scene\\n\",\n",
    "    \"            rect = plt.Rectangle((80, 160), 64, 40, fill=False, color='orange', linewidth=2)\\n\",\n",
    "    \"            ax_orig.add_patch(rect)\\n\",\n",
    "    \"            ax_orig.text(112, 210, 'Context', ha='center', color='orange', fontweight='bold')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Plot Grad-CAM overlay\\n\",\n",
    "    \"        ax_gradcam = axes[1, idx]\\n\",\n",
    "    \"        ax_gradcam.imshow(original_image, cmap='gray', alpha=0.4)\\n\",\n",
    "    \"        im = ax_gradcam.imshow(gradcam_heatmap, cmap='hot', alpha=0.6)\\n\",\n",
    "    \"        ax_gradcam.set_title(f'Grad-CAM Heatmap\\\\nSaliency: {gradcam_heatmap.max():.3f}', \\n\",\n",
    "    \"                            fontsize=12, fontweight='bold')\\n\",\n",
    "    \"        ax_gradcam.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Add colorbar for the last subplot\\n\",\n",
    "    \"        if idx == len(scenarios) - 1:\\n\",\n",
    "    \"            cbar = plt.colorbar(im, ax=ax_gradcam, fraction=0.046, pad=0.04)\\n\",\n",
    "    \"            cbar.set_label('Attention Intensity', fontsize=10)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.suptitle('Grad-CAM Visualization for Vision Encoder Analysis', \\n\",\n",
    "    \"                 fontsize=16, fontweight='bold', y=0.98)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    return fig\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create Grad-CAM visualizations\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from scipy import ndimage\\n\",\n",
    "    \"    fig = create_gradcam_visualization()\\n\",\n",
    "    \"    plt.savefig(output_dir / 'gradcam_visualizations.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"    plt.savefig(docs_dir / 'gradcam_visualizations.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Grad-CAM visualizations saved to:\\\")\\n\",\n",
    "    \"    print(f\\\"  Analysis: {output_dir / 'gradcam_visualizations.png'}\\\")\\n\",\n",
    "    \"    print(f\\\"  Documentation: {docs_dir / 'gradcam_visualizations.png'}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"SciPy not available for Grad-CAM visualization. Install with: pip install scipy\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Audio Attention Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create audio attention visualizations\\n\",\n",
    "    \"def create_audio_attention_visualization():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create audio attention maps for speech analysis.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Mock audio data and attention\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Simulate audio spectrogram data\\n\",\n",
    "    \"    time_steps = 100\\n\",\n",
    "    \"    freq_bins = 80\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create mock spectrogram\\n\",\n",
    "    \"    spectrogram = np.random.rand(freq_bins, time_steps)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add some structure to make it look more realistic\\n\",\n",
    "    \"    for t in range(time_steps):\\n\",\n",
    "    \"        # Add formant-like structure\\n\",\n",
    "    \"        if t % 20 < 10:  # Speech segments\\n\",\n",
    "    \"            spectrogram[10:20, t] *= 2  # First formant\\n\",\n",
    "    \"            spectrogram[30:40, t] *= 1.5  # Second formant\\n\",\n",
    "    \"            spectrogram[50:60, t] *= 1.2  # Third formant\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create attention patterns for different aspects\\n\",\n",
    "    \"    pitch_attention = np.zeros((freq_bins, time_steps))\\n\",\n",
    "    \"    rhythm_attention = np.zeros((freq_bins, time_steps))\\n\",\n",
    "    \"    prosody_attention = np.zeros((freq_bins, time_steps))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Pitch attention (focus on fundamental frequency)\\n\",\n",
    "    \"    pitch_attention[5:25, :] = np.random.rand(20, time_steps) * 0.8\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Rhythm attention (temporal patterns)\\n\",\n",
    "    \"    for t in range(0, time_steps, 15):\\n\",\n",
    "    \"        rhythm_attention[:, t:t+5] = np.random.rand(freq_bins, min(5, time_steps-t)) * 0.6\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Prosody attention (higher frequencies)\\n\",\n",
    "    \"    prosody_attention[40:70, :] = np.random.rand(30, time_steps) * 0.7\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create visualization\\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 1: Original Spectrogram\\n\",\n",
    "    \"    ax1 = axes[0, 0]\\n\",\n",
    "    \"    im1 = ax1.imshow(spectrogram, cmap='viridis', aspect='auto', origin='lower')\\n\",\n",
    "    \"    ax1.set_title('Audio Spectrogram\\\\n\\\"Oh great, another meeting\\\"', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax1.set_xlabel('Time Frames')\\n\",\n",
    "    \"    ax1.set_ylabel('Frequency Bins')\\n\",\n",
    "    \"    plt.colorbar(im1, ax=ax1, label='Magnitude')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add speech segment annotations\\n\",\n",
    "    \"    segments = [(0, 25, 'Oh'), (25, 45, 'great'), (45, 65, 'another'), (65, 100, 'meeting')]\\n\",\n",
    "    \"    for start, end, word in segments:\\n\",\n",
    "    \"        ax1.axvline(start, color='white', linestyle='--', alpha=0.7)\\n\",\n",
    "    \"        ax1.text((start + end) / 2, freq_bins - 5, word, ha='center', \\n\",\n",
    "    \"                color='white', fontweight='bold', fontsize=10)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 2: Pitch Attention\\n\",\n",
    "    \"    ax2 = axes[0, 1] \\n\",\n",
    "    \"    im2 = ax2.imshow(pitch_attention, cmap='Reds', aspect='auto', origin='lower', alpha=0.8)\\n\",\n",
    "    \"    ax2.imshow(spectrogram, cmap='gray', aspect='auto', origin='lower', alpha=0.3)\\n\",\n",
    "    \"    ax2.set_title('Pitch Attention\\\\n(Fundamental Frequency Focus)', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax2.set_xlabel('Time Frames')\\n\",\n",
    "    \"    ax2.set_ylabel('Frequency Bins')\\n\",\n",
    "    \"    plt.colorbar(im2, ax=ax2, label='Attention Weight')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 3: Rhythm Attention\\n\",\n",
    "    \"    ax3 = axes[1, 0]\\n\",\n",
    "    \"    im3 = ax3.imshow(rhythm_attention, cmap='Blues', aspect='auto', origin='lower', alpha=0.8)\\n\",\n",
    "    \"    ax3.imshow(spectrogram, cmap='gray', aspect='auto', origin='lower', alpha=0.3)\\n\",\n",
    "    \"    ax3.set_title('Rhythm Attention\\\\n(Temporal Pattern Focus)', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax3.set_xlabel('Time Frames')\\n\",\n",
    "    \"    ax3.set_ylabel('Frequency Bins')\\n\",\n",
    "    \"    plt.colorbar(im3, ax=ax3, label='Attention Weight')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot 4: Prosody Attention\\n\",\n",
    "    \"    ax4 = axes[1, 1]\\n\",\n",
    "    \"    im4 = ax4.imshow(prosody_attention, cmap='Greens', aspect='auto', origin='lower', alpha=0.8)\\n\",\n",
    "    \"    ax4.imshow(spectrogram, cmap='gray', aspect='auto', origin='lower', alpha=0.3)\\n\",\n",
    "    \"    ax4.set_title('Prosody Attention\\\\n(Emotional Tone Focus)', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"    ax4.set_xlabel('Time Frames')\\n\",\n",
    "    \"    ax4.set_ylabel('Frequency Bins')\\n\",\n",
    "    \"    plt.colorbar(im4, ax=ax4, label='Attention Weight')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    return fig\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create audio attention visualization\\n\",\n",
    "    \"fig = create_audio_attention_visualization()\\n\",\n",
    "    \"plt.savefig(output_dir / 'audio_attention_maps.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.savefig(docs_dir / 'audio_attention_maps.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Audio attention maps saved to:\\\")\\n\",\n",
    "    \"print(f\\\"  Analysis: {output_dir / 'audio_attention_maps.png'}\\\")\\n\",\n",
    "    \"print(f\\\"  Documentation: {docs_dir / 'audio_attention_maps.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Interactive Embeddings Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create interactive embeddings visualization\\n\",\n",
    "    \"def create_interactive_embeddings():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create interactive embeddings visualization using t-SNE/UMAP.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Generate mock high-dimensional embeddings\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    n_samples = 1000\\n\",\n",
    "    \"    embedding_dim = 768\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create mock embeddings for different tasks and datasets\\n\",\n",
    "    \"    embeddings = np.random.randn(n_samples, embedding_dim)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create labels and metadata\\n\",\n",
    "    \"    tasks = np.random.choice(['Sarcasm', 'Fact_Check', 'Paraphrase'], n_samples, p=[0.5, 0.3, 0.2])\\n\",\n",
    "    \"    datasets = np.random.choice(['SARC', 'MMSD2', 'FEVER', 'LIAR', 'ParaNMT'], n_samples)\\n\",\n",
    "    \"    predictions = np.random.choice(['Correct', 'Incorrect'], n_samples, p=[0.75, 0.25])\\n\",\n",
    "    \"    confidence = np.random.uniform(0.4\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
