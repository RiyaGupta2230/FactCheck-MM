{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf4278",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# FactCheck-MM Data Exploration\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Overview\\n\",\n",
    "    \"This notebook provides comprehensive exploration of all FactCheck-MM datasets including:\\n\",\n",
    "    \"- Dataset statistics and distributions\\n\",\n",
    "    \"- Multimodal content analysis\\n\",\n",
    "    \"- Data quality assessment\\n\",\n",
    "    \"- Preprocessing recommendations\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Method\\n\",\n",
    "    \"We analyze 12 datasets across three tasks:\\n\",\n",
    "    \"- **Sarcasm Detection**: SARC, MMSD2, MUStARD, UR-FUNNY, SarcNet, Headlines\\n\",\n",
    "    \"- **Paraphrasing**: ParaNMT-5M, MRPC, Quora\\n\",\n",
    "    \"- **Fact Verification**: FEVER, LIAR, New Headlines\\n\",\n",
    "    \"\\n\",\n",
    "    \"Each dataset is examined for modality availability, label distribution, and content characteristics.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Setup and imports\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add project root to path\\n\",\n",
    "    \"project_root = Path().cwd().parent if Path().cwd().name == 'notebooks' else Path().cwd()\\n\",\n",
    "    \"sys.path.insert(0, str(project_root))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project utilities\\n\",\n",
    "    \"from shared.datasets.unified_loader import UnifiedDatasetLoader\\n\",\n",
    "    \"from shared.utils.visualization import create_class_distribution_plot, create_text_length_histogram\\n\",\n",
    "    \"from shared.utils.metrics import calculate_dataset_statistics\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set style\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create output directory\\n\",\n",
    "    \"output_dir = project_root / 'outputs' / 'notebooks'\\n\",\n",
    "    \"output_dir.mkdir(parents=True, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Project root: {project_root}\\\")\\n\",\n",
    "    \"print(f\\\"Output directory: {output_dir}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Dataset Overview\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load dataset configurations\\n\",\n",
    "    \"dataset_configs = {\\n\",\n",
    "    \"    'sarcasm_detection': {\\n\",\n",
    "    \"        'datasets': ['sarc', 'mmsd2', 'mustard', 'ur_funny', 'sarcnet', 'sarcasm_headlines'],\\n\",\n",
    "    \"        'task_type': 'classification',\\n\",\n",
    "    \"        'num_classes': 2,\\n\",\n",
    "    \"        'modalities': ['text', 'audio', 'image', 'video']\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'paraphrasing': {\\n\",\n",
    "    \"        'datasets': ['paranmt', 'mrpc', 'quora'],\\n\",\n",
    "    \"        'task_type': 'generation',\\n\",\n",
    "    \"        'modalities': ['text']\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'fact_verification': {\\n\",\n",
    "    \"        'datasets': ['fever', 'liar', 'new_headlines'],\\n\",\n",
    "    \"        'task_type': 'classification',\\n\",\n",
    "    \"        'num_classes': 3,\\n\",\n",
    "    \"        'modalities': ['text']\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize dataset loader\\n\",\n",
    "    \"data_loader = UnifiedDatasetLoader(project_root / 'data')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"FactCheck-MM Dataset Configuration:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"for task, config in dataset_configs.items():\\n\",\n",
    "    \"    print(f\\\"\\\\n{task.replace('_', ' ').title()}:\\\")\\n\",\n",
    "    \"    print(f\\\"  Datasets: {', '.join(config['datasets'])}\\\")\\n\",\n",
    "    \"    print(f\\\"  Task Type: {config['task_type']}\\\")\\n\",\n",
    "    \"    print(f\\\"  Modalities: {', '.join(config['modalities'])}\\\")\\n\",\n",
    "    \"    if 'num_classes' in config:\\n\",\n",
    "    \"        print(f\\\"  Classes: {config['num_classes']}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Dataset Statistics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load and analyze each dataset\\n\",\n",
    "    \"dataset_stats = {}\\n\",\n",
    "    \"all_datasets_info = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for task, config in dataset_configs.items():\\n\",\n",
    "    \"    print(f\\\"\\\\nAnalyzing {task.replace('_', ' ').title()} datasets...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    task_stats = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for dataset_name in config['datasets']:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            # Load dataset splits\\n\",\n",
    "    \"            train_data = data_loader.load_dataset(dataset_name, 'train', sample_size=1000)\\n\",\n",
    "    \"            val_data = data_loader.load_dataset(dataset_name, 'val', sample_size=300) \\n\",\n",
    "    \"            test_data = data_loader.load_dataset(dataset_name, 'test', sample_size=300)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Calculate statistics\\n\",\n",
    "    \"            stats = {\\n\",\n",
    "    \"                'dataset': dataset_name,\\n\",\n",
    "    \"                'task': task,\\n\",\n",
    "    \"                'train_size': len(train_data) if train_data is not None else 0,\\n\",\n",
    "    \"                'val_size': len(val_data) if val_data is not None else 0,\\n\",\n",
    "    \"                'test_size': len(test_data) if test_data is not None else 0,\\n\",\n",
    "    \"                'total_size': 0,\\n\",\n",
    "    \"                'modalities_available': [],\\n\",\n",
    "    \"                'avg_text_length': 0,\\n\",\n",
    "    \"                'label_distribution': {}\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Analyze train split in detail\\n\",\n",
    "    \"            if train_data is not None and len(train_data) > 0:\\n\",\n",
    "    \"                sample = train_data[0]\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Check available modalities\\n\",\n",
    "    \"                if 'text' in sample and sample['text']:\\n\",\n",
    "    \"                    stats['modalities_available'].append('text')\\n\",\n",
    "    \"                if 'image_path' in sample or 'image' in sample:\\n\",\n",
    "    \"                    stats['modalities_available'].append('image')\\n\",\n",
    "    \"                if 'audio_path' in sample or 'audio' in sample:\\n\",\n",
    "    \"                    stats['modalities_available'].append('audio')\\n\",\n",
    "    \"                if 'video_path' in sample or 'video' in sample:\\n\",\n",
    "    \"                    stats['modalities_available'].append('video')\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Calculate text statistics\\n\",\n",
    "    \"                if 'text' in sample:\\n\",\n",
    "    \"                    text_lengths = [len(item.get('text', '').split()) for item in train_data[:100]]\\n\",\n",
    "    \"                    stats['avg_text_length'] = np.mean(text_lengths) if text_lengths else 0\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Label distribution\\n\",\n",
    "    \"                if 'label' in sample:\\n\",\n",
    "    \"                    labels = [item.get('label', 0) for item in train_data[:100]]\\n\",\n",
    "    \"                    unique, counts = np.unique(labels, return_counts=True)\\n\",\n",
    "    \"                    stats['label_distribution'] = dict(zip(unique.astype(str), counts.tolist()))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            stats['total_size'] = stats['train_size'] + stats['val_size'] + stats['test_size']\\n\",\n",
    "    \"            task_stats[dataset_name] = stats\\n\",\n",
    "    \"            all_datasets_info.append(stats)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            print(f\\\"  âœ“ {dataset_name}: {stats['total_size']:,} samples, {stats['modalities_available']}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"  âœ— {dataset_name}: Error - {e}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"    dataset_stats[task] = task_stats\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create summary DataFrame\\n\",\n",
    "    \"df_summary = pd.DataFrame(all_datasets_info)\\n\",\n",
    "    \"print(f\\\"\\\\nLoaded {len(df_summary)} datasets successfully\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display dataset summary table\\n\",\n",
    "    \"summary_display = df_summary[['dataset', 'task', 'total_size', 'modalities_available', 'avg_text_length']].copy()\\n\",\n",
    "    \"summary_display['modalities'] = summary_display['modalities_available'].apply(lambda x: ', '.join(x))\\n\",\n",
    "    \"summary_display = summary_display.drop('modalities_available', axis=1)\\n\",\n",
    "    \"summary_display['avg_text_length'] = summary_display['avg_text_length'].round(1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Dataset Summary:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 80)\\n\",\n",
    "    \"print(summary_display.to_string(index=False))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save summary\\n\",\n",
    "    \"summary_display.to_csv(output_dir / 'dataset_summary.csv', index=False)\\n\",\n",
    "    \"print(f\\\"\\\\nSummary saved to: {output_dir / 'dataset_summary.csv'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Dataset Size Visualization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create dataset size visualization\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(15, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Overall dataset sizes\\n\",\n",
    "    \"ax1 = axes[0, 0]\\n\",\n",
    "    \"datasets = df_summary['dataset'].tolist()\\n\",\n",
    "    \"sizes = df_summary['total_size'].tolist()\\n\",\n",
    "    \"colors = plt.cm.Set3(np.linspace(0, 1, len(datasets)))\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars = ax1.bar(range(len(datasets)), sizes, color=colors)\\n\",\n",
    "    \"ax1.set_title('Dataset Sizes', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax1.set_xlabel('Datasets')\\n\",\n",
    "    \"ax1.set_ylabel('Number of Samples')\\n\",\n",
    "    \"ax1.set_xticks(range(len(datasets)))\\n\",\n",
    "    \"ax1.set_xticklabels(datasets, rotation=45, ha='right')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels on bars\\n\",\n",
    "    \"for bar, size in zip(bars, sizes):\\n\",\n",
    "    \"    height = bar.get_height()\\n\",\n",
    "    \"    ax1.text(bar.get_x() + bar.get_width()/2., height + max(sizes)*0.01,\\n\",\n",
    "    \"             f'{int(size):,}', ha='center', va='bottom', fontsize=8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Dataset sizes by task\\n\",\n",
    "    \"ax2 = axes[0, 1]\\n\",\n",
    "    \"task_sizes = df_summary.groupby('task')['total_size'].sum()\\n\",\n",
    "    \"task_colors = ['#FF9999', '#66B2FF', '#99FF99']\\n\",\n",
    "    \"\\n\",\n",
    "    \"wedges, texts, autotexts = ax2.pie(task_sizes.values, labels=task_sizes.index, \\n\",\n",
    "    \"                                   autopct='%1.1f%%', colors=task_colors, startangle=90)\\n\",\n",
    "    \"ax2.set_title('Total Samples by Task', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Modality availability\\n\",\n",
    "    \"ax3 = axes[1, 0]\\n\",\n",
    "    \"modality_counts = {}\\n\",\n",
    "    \"for _, row in df_summary.iterrows():\\n\",\n",
    "    \"    for modality in row['modalities_available']:\\n\",\n",
    "    \"        modality_counts[modality] = modality_counts.get(modality, 0) + 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"modalities = list(modality_counts.keys())\\n\",\n",
    "    \"counts = list(modality_counts.values())\\n\",\n",
    "    \"bars = ax3.bar(modalities, counts, color=['#FFB366', '#66FFB2', '#B366FF', '#FF66B3'])\\n\",\n",
    "    \"ax3.set_title('Modality Availability Across Datasets', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax3.set_xlabel('Modalities')\\n\",\n",
    "    \"ax3.set_ylabel('Number of Datasets')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bar, count in zip(bars, counts):\\n\",\n",
    "    \"    height = bar.get_height()\\n\",\n",
    "    \"    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.05,\\n\",\n",
    "    \"             str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Text length distribution\\n\",\n",
    "    \"ax4 = axes[1, 1]\\n\",\n",
    "    \"text_lengths = df_summary['avg_text_length'].dropna()\\n\",\n",
    "    \"ax4.hist(text_lengths, bins=10, color='lightblue', alpha=0.7, edgecolor='black')\\n\",\n",
    "    \"ax4.set_title('Average Text Length Distribution', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"ax4.set_xlabel('Average Words per Sample')\\n\",\n",
    "    \"ax4.set_ylabel('Number of Datasets')\\n\",\n",
    "    \"ax4.axvline(text_lengths.mean(), color='red', linestyle='--', \\n\",\n",
    "    \"            label=f'Mean: {text_lengths.mean():.1f} words')\\n\",\n",
    "    \"ax4.legend()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig(output_dir / 'dataset_overview.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Dataset overview visualization saved to: {output_dir / 'dataset_overview.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Sample Data Exploration\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display sample data from each task\\n\",\n",
    "    \"print(\\\"Sample Data from Each Task:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for task, config in dataset_configs.items():\\n\",\n",
    "    \"    print(f\\\"\\\\n{task.replace('_', ' ').title()}:\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 40)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get first available dataset for this task\\n\",\n",
    "    \"    sample_dataset = None\\n\",\n",
    "    \"    for dataset_name in config['datasets']:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            sample_data = data_loader.load_dataset(dataset_name, 'train', sample_size=5)\\n\",\n",
    "    \"            if sample_data and len(sample_data) > 0:\\n\",\n",
    "    \"                sample_dataset = dataset_name\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"        except:\\n\",\n",
    "    \"            continue\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if sample_dataset and sample_data:\\n\",\n",
    "    \"        print(f\\\"Dataset: {sample_dataset}\\\")\\n\",\n",
    "    \"        print(f\\\"Sample size: {len(sample_data)}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Show first few samples\\n\",\n",
    "    \"        for i, sample in enumerate(sample_data[:3]):\\n\",\n",
    "    \"            print(f\\\"\\\\nSample {i+1}:\\\")\\n\",\n",
    "    \"            if 'text' in sample:\\n\",\n",
    "    \"                text = sample['text'][:200] + \\\"...\\\" if len(sample['text']) > 200 else sample['text']\\n\",\n",
    "    \"                print(f\\\"  Text: {text}\\\")\\n\",\n",
    "    \"            if 'label' in sample:\\n\",\n",
    "    \"                print(f\\\"  Label: {sample['label']}\\\")\\n\",\n",
    "    \"            if 'paraphrase' in sample:\\n\",\n",
    "    \"                paraphrase = sample['paraphrase'][:200] + \\\"...\\\" if len(sample['paraphrase']) > 200 else sample['paraphrase']\\n\",\n",
    "    \"                print(f\\\"  Paraphrase: {paraphrase}\\\")\\n\",\n",
    "    \"            if 'evidence' in sample:\\n\",\n",
    "    \"                evidence = sample['evidence'][:150] + \\\"...\\\" if len(sample['evidence']) > 150 else sample['evidence']\\n\",\n",
    "    \"                print(f\\\"  Evidence: {evidence}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Show available modalities\\n\",\n",
    "    \"            modalities = []\\n\",\n",
    "    \"            for mod in ['text', 'image', 'audio', 'video']:\\n\",\n",
    "    \"                if f'{mod}_path' in sample or mod in sample:\\n\",\n",
    "    \"                    modalities.append(mod)\\n\",\n",
    "    \"            if modalities:\\n\",\n",
    "    \"                print(f\\\"  Available modalities: {', '.join(modalities)}\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"No sample data available for {task}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Label Distribution Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze label distributions for classification tasks\\n\",\n",
    "    \"classification_tasks = ['sarcasm_detection', 'fact_verification']\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axes = plt.subplots(len(classification_tasks), 1, figsize=(12, 8))\\n\",\n",
    "    \"if len(classification_tasks) == 1:\\n\",\n",
    "    \"    axes = [axes]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for idx, task in enumerate(classification_tasks):\\n\",\n",
    "    \"    ax = axes[idx]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Collect label distributions for this task\\n\",\n",
    "    \"    task_data = df_summary[df_summary['task'] == task]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(task_data) > 0:\\n\",\n",
    "    \"        datasets = task_data['dataset'].tolist()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create stacked bar chart for label distributions\\n\",\n",
    "    \"        label_data = {}\\n\",\n",
    "    \"        all_labels = set()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for _, row in task_data.iterrows():\\n\",\n",
    "    \"            if row['label_distribution']:\\n\",\n",
    "    \"                label_data[row['dataset']] = row['label_distribution']\\n\",\n",
    "    \"                all_labels.update(row['label_distribution'].keys())\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if label_data:\\n\",\n",
    "    \"            all_labels = sorted(list(all_labels))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Prepare data for stacked bar chart\\n\",\n",
    "    \"            bottom = np.zeros(len(datasets))\\n\",\n",
    "    \"            colors = plt.cm.Set3(np.linspace(0, 1, len(all_labels)))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            for label_idx, label in enumerate(all_labels):\\n\",\n",
    "    \"                values = []\\n\",\n",
    "    \"                for dataset in datasets:\\n\",\n",
    "    \"                    if dataset in label_data:\\n\",\n",
    "    \"                        values.append(label_data[dataset].get(label, 0))\\n\",\n",
    "    \"                    else:\\n\",\n",
    "    \"                        values.append(0)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                ax.bar(datasets, values, bottom=bottom, label=f'Label {label}', \\n\",\n",
    "    \"                       color=colors[label_idx], alpha=0.8)\\n\",\n",
    "    \"                bottom += values\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            ax.set_title(f'{task.replace(\\\"_\\\", \\\" \\\").title()} - Label Distribution', \\n\",\n",
    "    \"                        fontsize=14, fontweight='bold')\\n\",\n",
    "    \"            ax.set_xlabel('Datasets')\\n\",\n",
    "    \"            ax.set_ylabel('Number of Samples')\\n\",\n",
    "    \"            ax.legend()\\n\",\n",
    "    \"            ax.tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            ax.text(0.5, 0.5, f'No label distribution data for {task}', \\n\",\n",
    "    \"                   ha='center', va='center', transform=ax.transAxes)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        ax.text(0.5, 0.5, f'No data available for {task}', \\n\",\n",
    "    \"               ha='center', va='center', transform=ax.transAxes)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.savefig(output_dir / 'label_distributions.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Label distribution visualization saved to: {output_dir / 'label_distributions.png'}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Text Analysis and Word Clouds\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Text analysis and word cloud generation\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from wordcloud import WordCloud\\n\",\n",
    "    \"    from collections import Counter\\n\",\n",
    "    \"    import re\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Function to clean text\\n\",\n",
    "    \"    def clean_text(text):\\n\",\n",
    "    \"        if not isinstance(text, str):\\n\",\n",
    "    \"            return \\\"\\\"\\n\",\n",
    "    \"        # Remove URLs, mentions, hashtags, and special characters\\n\",\n",
    "    \"        text = re.sub(r'http\\\\S+', '', text)\\n\",\n",
    "    \"        text = re.sub(r'@\\\\w+', '', text)\\n\",\n",
    "    \"        text = re.sub(r'#\\\\w+', '', text)\\n\",\n",
    "    \"        text = re.sub(r'[^a-zA-Z\\\\s]', '', text)\\n\",\n",
    "    \"        return text.lower().strip()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Collect text data from different tasks\\n\",\n",
    "    \"    task_texts = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for task, config in dataset_configs.items():\\n\",\n",
    "    \"        all_texts = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for dataset_name in config['datasets'][:2]:  # Limit to first 2 datasets per task\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                sample_data = data_loader.load_dataset(dataset_name, 'train', sample_size=200)\\n\",\n",
    "    \"                if sample_data:\\n\",\n",
    "    \"                    for item in sample_data:\\n\",\n",
    "    \"                        if 'text' in item and item['text']:\\n\",\n",
    "    \"                            cleaned_text = clean_text(item['text'])\\n\",\n",
    "    \"                            if cleaned_text:\\n\",\n",
    "    \"                                all_texts.append(cleaned_text)\\n\",\n",
    "    \"            except:\\n\",\n",
    "    \"                continue\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if all_texts:\\n\",\n",
    "    \"            task_texts[task] = ' '.join(all_texts)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate word clouds\\n\",\n",
    "    \"    if task_texts:\\n\",\n",
    "    \"        fig, axes = plt.subplots(1, len(task_texts), figsize=(5*len(task_texts), 5))\\n\",\n",
    "    \"        if len(task_texts) == 1:\\n\",\n",
    "    \"            axes = [axes]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for idx, (task, text) in enumerate(task_texts.items()):\\n\",\n",
    "    \"            if len(axes) > idx:\\n\",\n",
    "    \"                wordcloud = WordCloud(width=400, height=300, \\n\",\n",
    "    \"                                     background_color='white',\\n\",\n",
    "    \"                                     max_words=100,\\n\",\n",
    "    \"                                     colormap='viridis').generate(text)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                axes[idx].imshow(wordcloud, interpolation='bilinear')\\n\",\n",
    "    \"                axes[idx].set_title(f'{task.replace(\\\"_\\\", \\\" \\\").title()}\\\\nWord Cloud', \\n\",\n",
    "    \"                                   fontsize=12, fontweight='bold')\\n\",\n",
    "    \"                axes[idx].axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.savefig(output_dir / 'word_clouds.png', dpi=300, bbox_inches='tight')\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"Word clouds saved to: {output_dir / 'word_clouds.png'}\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"No text data available for word cloud generation\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"WordCloud library not available. Install with: pip install wordcloud\\\")\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"Error generating word clouds: {e}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"Based on the data exploration, we have identified:\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Dataset Characteristics\\n\",\n",
    "    \"- **Total Datasets**: 12 datasets across 3 tasks\\n\",\n",
    "    \"- **Multimodal Coverage**: Text is universal, audio/image/video available in select datasets\\n\",\n",
    "    \"- **Size Variation**: Significant variation in dataset sizes (from hundreds to millions of samples)\\n\",\n",
    "    \"- **Text Complexity**: Average text length varies by task type and domain\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Task Distribution\\n\",\n",
    "    \"- **Sarcasm Detection**: 6 datasets with varying multimodal support\\n\",\n",
    "    \"- **Paraphrasing**: 3 large text-only datasets\\n\",\n",
    "    \"- **Fact Verification**: 3 datasets with claim-evidence pairs\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Modality Analysis\\n\",\n",
    "    \"- Text modality: Present in all datasets\\n\",\n",
    "    \"- Audio modality: Available in video-based sarcasm datasets\\n\",\n",
    "    \"- Image modality: Present in social media and multimodal datasets\\n\",\n",
    "    \"- Video modality: Limited to specific sarcasm detection datasets\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Insights\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Data Imbalance**: Some datasets show significant class imbalance requiring careful handling\\n\",\n",
    "    \"2. **Multimodal Opportunities**: Video-based datasets offer rich multimodal learning opportunities\\n\",\n",
    "    \"3. **Text Diversity**: Different domains (social media, news, academic) provide diverse text patterns\\n\",\n",
    "    \"4. **Preprocessing Needs**: Text cleaning, standardization, and modality alignment required\\n\",\n",
    "    \"5. **Evaluation Considerations**: Varying dataset sizes suggest weighted evaluation strategies\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Recommendations\\n\",\n",
    "    \"- Implement stratified sampling for imbalanced datasets\\n\",\n",
    "    \"- Use data augmentation for smaller datasets\\n\",\n",
    "    \"- Standardize text preprocessing across all datasets\\n\",\n",
    "    \"- Develop modality-specific preprocessing pipelines\\n\",\n",
    "    \"- Consider dataset-specific evaluation metrics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save comprehensive dataset analysis\\n\",\n",
    "    \"analysis_summary = {\\n\",\n",
    "    \"    'dataset_statistics': dataset_stats,\\n\",\n",
    "    \"    'total_datasets': len(df_summary),\\n\",\n",
    "    \"    'total_samples': int(df_summary['total_size'].sum()),\\n\",\n",
    "    \"    'task_distribution': df_summary.groupby('task')['dataset'].count().to_dict(),\\n\",\n",
    "    \"    'modality_coverage': modality_counts,\\n\",\n",
    "    \"    'average_text_length_by_task': df_summary.groupby('task')['avg_text_length'].mean().to_dict(),\\n\",\n",
    "    \"    'recommendations': [\\n\",\n",
    "    \"        \\\"Implement stratified sampling for imbalanced datasets\\\",\\n\",\n",
    "    \"        \\\"Use data augmentation for smaller datasets\\\", \\n\",\n",
    "    \"        \\\"Standardize text preprocessing across all datasets\\\",\\n\",\n",
    "    \"        \\\"Develop modality-specific preprocessing pipelines\\\",\\n\",\n",
    "    \"        \\\"Consider dataset-specific evaluation metrics\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save analysis\\n\",\n",
    "    \"with open(output_dir / 'dataset_analysis_summary.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(analysis_summary, f, indent=2, default=str)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nComplete dataset analysis saved to: {output_dir / 'dataset_analysis_summary.json'}\\\")\\n\",\n",
    "    \"print(f\\\"All visualizations saved to: {output_dir}\\\")\\n\",\n",
    "    \"print(\\\"\\\\nData exploration completed successfully! âœ“\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
