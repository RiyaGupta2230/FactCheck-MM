#!/usr/bin/env python3
"""
Full Pipeline Runner for FactCheck-MM
End-to-end execution: sarcasm detection ‚Üí paraphrasing ‚Üí fact verification
"""

import sys
import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from shared.utils.logging_utils import get_logger, setup_logging
from Config.base_config import BaseConfig


@dataclass
class PipelineConfig:
    """Configuration for full pipeline execution."""
    
    # Task selection
    run_sarcasm_detection: bool = True
    run_paraphrasing: bool = True
    run_fact_verification: bool = True
    
    # Data flow
    use_sarcasm_for_paraphrasing: bool = True
    use_paraphrases_for_verification: bool = True
    
    # Model settings
    sarcasm_model_path: Optional[str] = None
    paraphrase_model_path: Optional[str] = None
    fact_check_model_path: Optional[str] = None
    
    # Hardware
    device: str = "auto"
    batch_size: int = 16
    max_length: int = 512
    
    # Output
    save_intermediate_results: bool = True
    output_dir: str = "pipeline_outputs"
    experiment_name: str = "full_pipeline"


class FactCheckPipeline:
    """Complete FactCheck-MM pipeline orchestrator."""
    
    def __init__(self, config: PipelineConfig):
        """
        Initialize pipeline.
        
        Args:
            config: Pipeline configuration
        """
        self.config = config
        self.logger = get_logger("FactCheckPipeline")
        
        # Setup output directory
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Pipeline state
        self.results = {}
        self.intermediate_data = {}
        
        # Initialize task modules (lazy loading)
        self._sarcasm_detector = None
        self._paraphraser = None
        self._fact_verifier = None
        
        self.logger.info(f"Initialized FactCheck-MM pipeline: {config.experiment_name}")
    
    def _setup_device(self):
        """Setup compute device for pipeline."""
        import torch
        
        if self.config.device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                gpu_name = torch.cuda.get_device_name(0)
                self.logger.info(f"üéÆ Using GPU: {gpu_name}")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
                self.logger.info("üçé Using Apple Metal Performance Shaders")
            else:
                device = "cpu"
                self.logger.info("üíª Using CPU")
        else:
            device = self.config.device
            self.logger.info(f"üéØ Using specified device: {device}")
        
        self.device = device
        return device
    
    def _load_sarcasm_detector(self):
        """Load sarcasm detection model."""
        if self._sarcasm_detector is not None:
            return self._sarcasm_detector
        
        self.logger.info("üì• Loading sarcasm detection model...")
        
        try:
            from sarcasm_detection.models import MultimodalSarcasmModel
            from sarcasm_detection.evaluation import SarcasmEvaluator
            import torch
            
            if self.config.sarcasm_model_path:
                # Load trained model
                self.logger.info(f"Loading from checkpoint: {self.config.sarcasm_model_path}")
                checkpoint = torch.load(self.config.sarcasm_model_path, map_location=self.device)
                
                model_config = checkpoint.get('config', {})
                model = MultimodalSarcasmModel(model_config)
                model.load_state_dict(checkpoint['model_state_dict'])
                model.to(self.device)
                model.eval()
            else:
                # Use pre-trained or default model
                self.logger.info("Using default sarcasm detection model")
                model_config = {
                    'modalities': ['text'],  # Start with text-only for compatibility
                    'num_classes': 2,
                    'fusion_strategy': 'cross_modal_attention'
                }
                model = MultimodalSarcasmModel(model_config)
                model.to(self.device)
                model.eval()
            
            self._sarcasm_detector = {
                'model': model,
                'evaluator': SarcasmEvaluator(model)
            }
            
            self.logger.info("‚úÖ Sarcasm detection model loaded")
            return self._sarcasm_detector
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load sarcasm detection model: {e}")
            raise
    
    def _load_paraphraser(self):
        """Load paraphrasing model."""
        if self._paraphraser is not None:
            return self._paraphraser
        
        self.logger.info("üì• Loading paraphrasing model...")
        
        # Placeholder for paraphrasing model loading
        # This will be implemented when paraphrasing module is complete
        self.logger.warning("‚ö†Ô∏è Paraphrasing module not yet implemented - using placeholder")
        
        self._paraphraser = {
            'model': None,
            'generator': None
        }
        
        return self._paraphraser
    
    def _load_fact_verifier(self):
        """Load fact verification model."""
        if self._fact_verifier is not None:
            return self._fact_verifier
        
        self.logger.info("üì• Loading fact verification model...")
        
        # Placeholder for fact verification model loading
        # This will be implemented when fact_verification module is complete
        self.logger.warning("‚ö†Ô∏è Fact verification module not yet implemented - using placeholder")
        
        self._fact_verifier = {
            'model': None,
            'evaluator': None
        }
        
        return self._fact_verifier
    
    def run_sarcasm_detection(self, input_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Run sarcasm detection on input data.
        
        Args:
            input_data: List of samples with text/multimodal content
            
        Returns:
            Processed data with sarcasm predictions
        """
        if not self.config.run_sarcasm_detection:
            self.logger.info("‚è≠Ô∏è Skipping sarcasm detection")
            return input_data
        
        self.logger.info(f"üé≠ Running sarcasm detection on {len(input_data)} samples")
        
        detector = self._load_sarcasm_detector()
        
        # Process samples
        processed_data = []
        
        for i, sample in enumerate(input_data):
            try:
                # Extract text content
                text = sample.get('text', sample.get('claim', ''))
                
                if not text:
                    self.logger.warning(f"Sample {i}: No text content found")
                    sample['sarcasm_prediction'] = 0
                    sample['sarcasm_confidence'] = 0.0
                    processed_data.append(sample)
                    continue
                
                # Create input for sarcasm model
                model_input = {
                    'text': text,
                    'audio': sample.get('audio'),
                    'image': sample.get('image'),
                    'video': sample.get('video')
                }
                
                # Run prediction (placeholder implementation)
                # In practice, this would use the actual model
                import torch
                import random
                
                # Simulate sarcasm prediction
                sarcasm_prob = random.random()
                sarcasm_prediction = 1 if sarcasm_prob > 0.5 else 0
                
                # Add predictions to sample
                sample['sarcasm_prediction'] = sarcasm_prediction
                sample['sarcasm_confidence'] = sarcasm_prob
                sample['is_sarcastic'] = bool(sarcasm_prediction)
                
                processed_data.append(sample)
                
            except Exception as e:
                self.logger.error(f"‚ùå Error processing sample {i}: {e}")
                sample['sarcasm_prediction'] = 0
                sample['sarcasm_confidence'] = 0.0
                processed_data.append(sample)
        
        # Save intermediate results
        if self.config.save_intermediate_results:
            sarcasm_output = self.output_dir / "sarcasm_results.json"
            with open(sarcasm_output, 'w') as f:
                json.dump(processed_data, f, indent=2, default=str)
            
            self.logger.info(f"üíæ Sarcasm results saved to: {sarcasm_output}")
        
        self.results['sarcasm_detection'] = {
            'total_samples': len(processed_data),
            'sarcastic_samples': sum(1 for s in processed_data if s.get('is_sarcastic', False)),
            'avg_confidence': sum(s.get('sarcasm_confidence', 0) for s in processed_data) / len(processed_data)
        }
        
        self.logger.info(f"‚úÖ Sarcasm detection completed: {self.results['sarcasm_detection']['sarcastic_samples']}/{len(processed_data)} sarcastic")
        
        return processed_data
    
    def run_paraphrasing(self, input_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Run paraphrasing on input data.
        
        Args:
            input_data: Data from sarcasm detection
            
        Returns:
            Data with paraphrases
        """
        if not self.config.run_paraphrasing:
            self.logger.info("‚è≠Ô∏è Skipping paraphrasing")
            return input_data
        
        self.logger.info(f"üîÑ Running paraphrasing on {len(input_data)} samples")
        
        paraphraser = self._load_paraphraser()
        
        # Process samples
        processed_data = []
        
        for i, sample in enumerate(input_data):
            try:
                text = sample.get('text', sample.get('claim', ''))
                
                if not text:
                    self.logger.warning(f"Sample {i}: No text for paraphrasing")
                    sample['paraphrases'] = []
                    processed_data.append(sample)
                    continue
                
                # Generate paraphrases
                # Placeholder implementation - replace with actual paraphrasing
                paraphrases = [
                    f"Paraphrase 1: {text}",
                    f"Alternative: {text}",
                    f"Rephrased: {text}"
                ]
                
                # Consider sarcasm context if available
                if self.config.use_sarcasm_for_paraphrasing and sample.get('is_sarcastic', False):
                    paraphrases = [f"[Non-sarcastic] {p}" for p in paraphrases]
                
                sample['paraphrases'] = paraphrases
                sample['num_paraphrases'] = len(paraphrases)
                
                processed_data.append(sample)
                
            except Exception as e:
                self.logger.error(f"‚ùå Error paraphrasing sample {i}: {e}")
                sample['paraphrases'] = []
                processed_data.append(sample)
        
        # Save intermediate results
        if self.config.save_intermediate_results:
            para_output = self.output_dir / "paraphrase_results.json"
            with open(para_output, 'w') as f:
                json.dump(processed_data, f, indent=2, default=str)
            
            self.logger.info(f"üíæ Paraphrase results saved to: {para_output}")
        
        self.results['paraphrasing'] = {
            'total_samples': len(processed_data),
            'total_paraphrases': sum(len(s.get('paraphrases', [])) for s in processed_data),
            'avg_paraphrases_per_sample': sum(len(s.get('paraphrases', [])) for s in processed_data) / len(processed_data)
        }
        
        self.logger.info(f"‚úÖ Paraphrasing completed: {self.results['paraphrasing']['total_paraphrases']} paraphrases generated")
        
        return processed_data
    
    def run_fact_verification(self, input_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Run fact verification on input data.
        
        Args:
            input_data: Data from paraphrasing
            
        Returns:
            Final results with fact-check verdicts
        """
        if not self.config.run_fact_verification:
            self.logger.info("‚è≠Ô∏è Skipping fact verification")
            return input_data
        
        self.logger.info(f"üîç Running fact verification on {len(input_data)} samples")
        
        fact_verifier = self._load_fact_verifier()
        
        # Process samples
        processed_data = []
        
        for i, sample in enumerate(input_data):
            try:
                # Get claims to verify
                claims_to_verify = []
                
                # Original claim
                original_claim = sample.get('text', sample.get('claim', ''))
                if original_claim:
                    claims_to_verify.append(original_claim)
                
                # Add paraphrases if enabled
                if self.config.use_paraphrases_for_verification:
                    paraphrases = sample.get('paraphrases', [])
                    claims_to_verify.extend(paraphrases)
                
                # Run fact verification on each claim
                verification_results = []
                
                for claim in claims_to_verify:
                    # Placeholder fact verification
                    import random
                    
                    verdict_score = random.random()
                    if verdict_score > 0.7:
                        verdict = "TRUE"
                    elif verdict_score > 0.4:
                        verdict = "PARTIALLY_TRUE"
                    else:
                        verdict = "FALSE"
                    
                    verification_results.append({
                        'claim': claim,
                        'verdict': verdict,
                        'confidence': verdict_score,
                        'evidence': f"Evidence for: {claim[:50]}..."
                    })
                
                # Aggregate results
                verdicts = [r['verdict'] for r in verification_results]
                confidences = [r['confidence'] for r in verification_results]
                
                sample['verification_results'] = verification_results
                sample['final_verdict'] = max(set(verdicts), key=verdicts.count)  # Most common verdict
                sample['avg_confidence'] = sum(confidences) / len(confidences) if confidences else 0.0
                sample['is_factual'] = sample['final_verdict'] in ['TRUE', 'PARTIALLY_TRUE']
                
                processed_data.append(sample)
                
            except Exception as e:
                self.logger.error(f"‚ùå Error verifying sample {i}: {e}")
                sample['verification_results'] = []
                sample['final_verdict'] = "UNKNOWN"
                sample['is_factual'] = False
                processed_data.append(sample)
        
        # Save final results
        final_output = self.output_dir / "final_results.json"
        with open(final_output, 'w') as f:
            json.dump(processed_data, f, indent=2, default=str)
        
        self.logger.info(f"üíæ Final results saved to: {final_output}")
        
        self.results['fact_verification'] = {
            'total_samples': len(processed_data),
            'factual_samples': sum(1 for s in processed_data if s.get('is_factual', False)),
            'avg_confidence': sum(s.get('avg_confidence', 0) for s in processed_data) / len(processed_data)
        }
        
        self.logger.info(f"‚úÖ Fact verification completed: {self.results['fact_verification']['factual_samples']}/{len(processed_data)} factual")
        
        return processed_data
    
    def run_full_pipeline(self, input_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Run complete pipeline on input data.
        
        Args:
            input_data: Input samples
            
        Returns:
            Pipeline results and metrics
        """
        self.logger.info(f"üöÄ Starting full FactCheck-MM pipeline on {len(input_data)} samples")
        start_time = time.time()
        
        # Setup device
        self._setup_device()
        
        # Stage 1: Sarcasm Detection
        stage1_start = time.time()
        data = self.run_sarcasm_detection(input_data)
        stage1_time = time.time() - stage1_start
        
        # Stage 2: Paraphrasing
        stage2_start = time.time()
        data = self.run_paraphrasing(data)
        stage2_time = time.time() - stage2_start
        
        # Stage 3: Fact Verification
        stage3_start = time.time()
        final_data = self.run_fact_verification(data)
        stage3_time = time.time() - stage3_start
        
        total_time = time.time() - start_time
        
        # Compile final results
        final_results = {
            'pipeline_config': self.config.__dict__,
            'execution_time': {
                'total_seconds': total_time,
                'sarcasm_detection_seconds': stage1_time,
                'paraphrasing_seconds': stage2_time,
                'fact_verification_seconds': stage3_time
            },
            'stage_results': self.results,
            'final_data': final_data,
            'summary': {
                'total_input_samples': len(input_data),
                'total_output_samples': len(final_data),
                'sarcastic_samples': self.results.get('sarcasm_detection', {}).get('sarcastic_samples', 0),
                'total_paraphrases': self.results.get('paraphrasing', {}).get('total_paraphrases', 0),
                'factual_samples': self.results.get('fact_verification', {}).get('factual_samples', 0)
            }
        }
        
        # Save complete results
        complete_output = self.output_dir / f"{self.config.experiment_name}_complete.json"
        with open(complete_output, 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        self.logger.info(f"üéâ Pipeline completed in {total_time:.2f}s")
        self.logger.info(f"üìä Results: {final_results['summary']}")
        self.logger.info(f"üíæ Complete results: {complete_output}")
        
        return final_results


def load_input_data(input_source: str) -> List[Dict[str, Any]]:
    """
    Load input data from various sources.
    
    Args:
        input_source: Path to input file or dataset name
        
    Returns:
        List of input samples
    """
    logger = get_logger("DataLoader")
    
    if input_source.endswith('.json'):
        # Load from JSON file
        with open(input_source, 'r') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            return data
        else:
            return [data]
    
    elif input_source.endswith('.csv'):
        # Load from CSV file
        import pandas as pd
        df = pd.read_csv(input_source)
        return df.to_dict('records')
    
    elif input_source in ['fever', 'liar', 'mmsd2', 'mustard', 'sarc']:
        # Load from FactCheck-MM datasets
        logger.info(f"Loading from FactCheck-MM dataset: {input_source}")
        
        if input_source in ['mmsd2', 'mustard']:
            from sarcasm_detection.data.unified_loader import UnifiedSarcasmLoader
            loader = UnifiedSarcasmLoader('data/', [input_source])
            _, _, test_data = loader.load_datasets()
            
            # Convert dataset to list of dicts
            samples = []
            for i in range(min(100, len(test_data))):  # Limit for demo
                sample = test_data[i]
                samples.append({
                    'text': sample.get('text', ''),
                    'label': sample.get('label', 0),
                    'source_dataset': input_source
                })
            
            return samples
        
        elif input_source in ['fever', 'liar']:
            # Placeholder for fact verification datasets
            logger.warning(f"Fact verification dataset {input_source} not yet implemented")
            return [
                {'claim': f'Sample claim from {input_source} dataset', 'source_dataset': input_source}
                for _ in range(10)
            ]
    
    else:
        # Create sample data
        logger.info("Creating sample test data")
        return [
            {
                'text': 'This is definitely the best implementation ever created.',
                'source': 'sample'
            },
            {
                'text': 'Climate change is causing unprecedented global warming.',
                'source': 'sample'
            },
            {
                'text': 'The latest AI model achieved 99.9% accuracy on all tasks.',
                'source': 'sample'
            }
        ]


def main():
    """Main entry point for pipeline execution."""
    parser = argparse.ArgumentParser(description="FactCheck-MM Full Pipeline Runner")
    
    # Task selection
    parser.add_argument('--tasks', default='sarcasm,paraphrase,fact_check',
                       help='Comma-separated tasks to run (sarcasm,paraphrase,fact_check)')
    
    # Input/Output
    parser.add_argument('--input', default='sample',
                       help='Input data source (file path or dataset name)')
    parser.add_argument('--output-dir', default='pipeline_outputs',
                       help='Output directory for results')
    parser.add_argument('--experiment-name', default=f'pipeline_{int(time.time())}',
                       help='Experiment name')
    
    # Model paths
    parser.add_argument('--sarcasm-model', 
                       help='Path to trained sarcasm detection model')
    parser.add_argument('--paraphrase-model',
                       help='Path to trained paraphrasing model')
    parser.add_argument('--fact-check-model',
                       help='Path to trained fact verification model')
    
    # Execution settings
    parser.add_argument('--device', default='auto',
                       help='Device to use (auto, cuda, mps, cpu)')
    parser.add_argument('--batch-size', type=int, default=16,
                       help='Batch size for processing')
    parser.add_argument('--no-save-intermediate', action='store_true',
                       help='Do not save intermediate results')
    
    # Pipeline flow
    parser.add_argument('--no-sarcasm-conditioning', action='store_true',
                       help='Do not use sarcasm results for paraphrasing')
    parser.add_argument('--no-paraphrase-verification', action='store_true',
                       help='Do not verify paraphrases')
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(experiment_name=args.experiment_name)
    logger = get_logger("PipelineRunner")
    
    try:
        # Parse tasks
        tasks = [t.strip() for t in args.tasks.split(',')]
        
        # Create pipeline config
        config = PipelineConfig(
            run_sarcasm_detection='sarcasm' in tasks,
            run_paraphrasing='paraphrase' in tasks,
            run_fact_verification='fact_check' in tasks,
            
            sarcasm_model_path=args.sarcasm_model,
            paraphrase_model_path=args.paraphrase_model,
            fact_check_model_path=args.fact_check_model,
            
            device=args.device,
            batch_size=args.batch_size,
            save_intermediate_results=not args.no_save_intermediate,
            
            use_sarcasm_for_paraphrasing=not args.no_sarcasm_conditioning,
            use_paraphrases_for_verification=not args.no_paraphrase_verification,
            
            output_dir=args.output_dir,
            experiment_name=args.experiment_name
        )
        
        # Load input data
        logger.info(f"üì• Loading input data from: {args.input}")
        input_data = load_input_data(args.input)
        logger.info(f"üìä Loaded {len(input_data)} samples")
        
        # Initialize and run pipeline
        pipeline = FactCheckPipeline(config)
        results = pipeline.run_full_pipeline(input_data)
        
        # Print summary
        logger.info("üéØ Pipeline Summary:")
        logger.info(f"   Total time: {results['execution_time']['total_seconds']:.2f}s")
        logger.info(f"   Input samples: {results['summary']['total_input_samples']}")
        logger.info(f"   Sarcastic samples: {results['summary']['sarcastic_samples']}")
        logger.info(f"   Generated paraphrases: {results['summary']['total_paraphrases']}")
        logger.info(f"   Factual samples: {results['summary']['factual_samples']}")
        
        logger.info("‚úÖ Pipeline execution completed successfully!")
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"üí• Pipeline failed: {e}")
        raise


if __name__ == "__main__":
    main()
